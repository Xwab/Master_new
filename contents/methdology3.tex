\chapter{低秩感知的混合精度多粒度量化策略}
完成层间秩分配只是降低 KV 缓存体积的第一步；若希望进一步压缩，就必须引入低比特量化。然而已有实验（见表~\ref{tab:value-quant-granularity}）表明：当 Key/Value 被粗粒度地统一压到 2-bit 时，模型困惑度急剧上升，而 Value 分支因语义表达更敏感，退化尤为明显，提示我们需要比逐通道更细的量化粒度。其次，一旦把量化与低秩裁剪结合，层与层之间保留的 rank 彼此不同，group-wise 量化的组大小往往无法整除各自的特征维度，导致细粒度量化难以落地。最后，即使解决了粒度问题，我们仍需在不同秩之间区别对待量化精度：低秩分解所暴露的奇异值能量差异意味着重要秩必须维持较高精度，而长尾秩则可采用更低比特。如何在这三重约束下，把“粗/细粒度量化”与“混合精度、低秩感知”协同起来，构成了本章方法的出发点。

\begin{table}[htbp]
    \centering
    \caption{不同量化粒度在 2-bit 下的困惑度（Perplexity）对比}
    \label{tab:value-quant-granularity}
    \begin{tabular}{lcc}
        \toprule
        \textbf{量化粒度} & \textbf{WikiText-2} & \textbf{PTB} \\
        \midrule
        通道级（channel-wise, 2-bit） & 514.59 & 2665.66 \\
        分组级（group-wise, 2-bit） & 11.17 & 20.87 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{量化基础与离群挑战}
在将 KV 缓存从浮点表示压缩到 $b$ 比特整数表示时，我们通常对每个量化单元（可对应整层、单个通道或一个 group）估计一个放缩系数 $s$ 与可选的零点 $z$，使得
\[
    q = \operatorname{clip}\!\left(\left\lfloor \frac{x}{s} + z \right\rceil, q_{\min}, q_{\max}\right), \qquad
    \hat{x} = s \cdot (q - z),
\]
其中 $x$ 与 $\hat{x}$ 分别表示原始与反量化值，$q$ 为整数码字。若 $z=0$ 并强制 $q_{\min}=-q_{\max}$，即得到\textbf{对称量化}；其实现简单、硬件友好，但需要数据分布在零点附近且正负范围大致相当。相比之下，\textbf{非对称量化}允许 $z\neq 0$，通常以数据的最小值对齐零点，从而更好地覆盖偏移分布，是移动端推理中常见的方案。

然而，无论采用哪种策略，离群值（outlier）都会显著放大量化误差。若采用单一尺度 $s$ 覆盖所有值，少量幅值巨大的 KV 元素会迫使尺度增大，从而让绝大多数常规值落在更粗的量化间隔内，最终抬高困惑度；若强行截断离群值，又会在反量化时产生饱和失真。图~\ref{fig:kv-outliers} 将展示我们在实际 KV 缓存中观测到的离群现象：少量特征维度的绝对值远高于其余维度，这正是粗粒度量化在 Value 分支上退化严重的根源，也进一步强调了需要结合细粒度划分与混合精度策略来抑制离群带来的损失。

\subsection{Hadamard 变换与离群抑制}
为缓解上述离群幅值导致的动态范围膨胀，一类常见手段是在量化前施加 Hadamard 变换。Hadamard 矩阵 $H_n\in\{\pm 1\}^{n\times n}$ 是一族正交矩阵，满足 $H_n H_n^\top = n I$，其变换可通过快速 Walsh–Hadamard 变换（FWHT）在 $O(n\log n)$ 时间内完成，不涉及乘法，仅需加减法与轻量的缩放。我们对单个量化单元的向量 $x\in\mathbb{R}^n$ 施加标准化后的变换
\[
    \tilde{x} = \frac{1}{\sqrt{n}} H_n x,
\]
然后在 $\tilde{x}$ 空间执行量化。由于 Hadamard 变换实质上把每一维度投影到 $\pm 1$ 组合的正交基上，原本集中于少数维度的能量会被“打散”至各通道，极大地降低单个维度的峰度（kurtosis），从而使可用的量化尺度更贴近大多数数值。推理阶段只需在反量化后施加同样的 Hadamard（其自身就是逆变换），即可还原到原始基底。

典型的 Hadamard 矩阵阶数为 $2^k$。例如，
\[
H_1 = [1],\qquad
H_2 = \begin{bmatrix}
1 & 1 \\
1 & -1
\end{bmatrix},\qquad
H_4 = \begin{bmatrix}
1 & 1 & 1 & 1 \\
1 & -1 & 1 & -1 \\
1 & 1 & -1 & -1 \\
1 & -1 & -1 & 1
\end{bmatrix},
\]
更高阶矩阵可递归构造 $H_{2n} = \begin{bmatrix} H_n & H_n \\ H_n & -H_n \end{bmatrix}$。这些 $\pm1$ 结构使得 FWHT 仅需加减法即可完成正交变换，十分适合硬件实现。

尽管 Hadamard 变换无法完全消除异常值，它显著降低了极端值对量化尺度的牵制，尤其适合配合细粒度 group-wise 方案使用：在每个组内独立执行 FWHT，可在可接受的算力成本下换取更平滑的值分布。但该方法也带来额外的访存与延迟开销，且若组大小与 Hadamard 阶数不匹配，需要通过零填充或子矩阵拼接来适配，这些实现细节将在后续方法部分讨论。

\section{低秩感知的混合精度量化}
从图~\ref{fig:energy_key}和图~\ref{fig:energy_value}中可以观察到，虽然前文提出的算法中的分解目标$S_kW_k$和$\bigl[S_{v,1}W_{v,1},...S_{v,G}W_{v,G}\bigr]$都具有低秩性，即大部分能量都只集中在前几个奇异值上，但是明显能够发现Key的低秩性要更好一些，长尾的奇异值几乎不占多少能量了。因此我们认为对于完全丢弃能量占比最小的秩的做法，对于Key来说损失的信息是可以接受的，而对于Value来说可能造成一些仍然比较有用的信息被丢弃掉了。因此在我们已经通过低秩降维的方式对模型KV缓存进行了压缩后，为了进一步压缩模型，我们将特征维度与量化结合时对Key和Value采用了不同的方法。

\subsection{Key：低秩压缩后的再量化}
对于Key分支，我们保留上一章得到的低秩裁剪结果。若原始维度为$d$，压缩后保留$r_{k,l}$个秩，其空间占用约为原始的一部分，定义低秩压缩率为$\rho_1 = r_{k,l}/d$（或等价的存储体积比）。在该基础上，我们先以16-bit精度存储压缩后的中间值，以保证Key主奇异向量的能量基本无损；随后进一步将这部分数据量化到更低比特，例如8-bit或4-bit，使量化压缩率达到$\rho_2 = 16/b$。最终Key侧的总体压缩率可近似写成$\rho_1 \cdot \rho_2$：其中$\rho_1$来自低秩裁剪带来的特征维降维，$\rho_2$来自位宽压缩。由于Key的尾部奇异值能量极低，额外的量化误差对注意力分数影响有限，因此可以大胆采用较低比特，以获得额外的内存收益，同时保持查询键的匹配精度。