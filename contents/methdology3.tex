\chapter{低秩感知的混合精度多粒度量化策略}
完成层间秩分配只是降低 KV 缓存体积的第一步；若希望进一步压缩，就必须引入低比特量化。然而已有实验（见表~\ref{tab:value-quant-granularity}）表明：当 Key/Value 被粗粒度地统一压到 2-bit 时，模型困惑度急剧上升，而 Value 分支因语义表达更敏感，退化尤为明显，提示我们需要比逐通道更细的量化粒度。其次，一旦把量化与低秩裁剪结合，层与层之间保留的 rank 彼此不同，group-wise 量化的组大小往往无法整除各自的特征维度，导致细粒度量化难以落地。最后，即使解决了粒度问题，我们仍需在不同秩之间区别对待量化精度：低秩分解所暴露的奇异值能量差异意味着重要秩必须维持较高精度，而长尾秩则可采用更低比特。如何在这三重约束下，把“粗/细粒度量化”与“混合精度、低秩感知”协同起来，构成了本章方法的出发点。

\begin{table}[htbp]
    \centering
    \caption{不同量化粒度在 2-bit 下的困惑度（Perplexity）对比}
    \label{tab:value-quant-granularity}
    \begin{tabular}{lcc}
        \toprule
        \textbf{量化粒度} & \textbf{WikiText-2} & \textbf{PTB} \\
        \midrule
        通道级（channel-wise, 2-bit） & 514.59 & 2665.66 \\
        分组级（group-wise, 2-bit） & 11.17 & 20.87 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{量化基础与离群挑战}
在将 KV 缓存从浮点表示压缩到 $b$ 比特整数表示时，我们通常对每个量化单元（可对应整层、单个通道或一个 group）估计一个放缩系数 $s$ 与可选的零点 $z$，使得
\[
    q = \operatorname{clip}\!\left(\left\lfloor \frac{x}{s} + z \right\rceil, q_{\min}, q_{\max}\right), \qquad
    \hat{x} = s \cdot (q - z),
\]
其中 $x$ 与 $\hat{x}$ 分别表示原始与反量化值，$q$ 为整数码字。若 $z=0$ 并强制 $q_{\min}=-q_{\max}$，即得到\textbf{对称量化}；其实现简单、硬件友好，但需要数据分布在零点附近且正负范围大致相当。相比之下，\textbf{非对称量化}允许 $z\neq 0$，通常以数据的最小值对齐零点，从而更好地覆盖偏移分布，是移动端推理中常见的方案。

然而，无论采用哪种策略，离群值（outlier）都会显著放大量化误差。若采用单一尺度 $s$ 覆盖所有值，少量幅值巨大的 KV 元素会迫使尺度增大，从而让绝大多数常规值落在更粗的量化间隔内，最终抬高困惑度；若强行截断离群值，又会在反量化时产生饱和失真。图~\ref{fig:kv-outliers} 将展示我们在实际 KV 缓存中观测到的离群现象：少量特征维度的绝对值远高于其余维度，这正是粗粒度量化在 Value 分支上退化严重的根源，也进一步强调了需要结合细粒度划分与混合精度策略来抑制离群带来的损失。

\subsection{Hadamard 变换与离群抑制}
为缓解上述离群幅值导致的动态范围膨胀，一类常见手段是在量化前施加 Hadamard 变换。Hadamard 矩阵 $H_n\in\{\pm 1\}^{n\times n}$ 是一族正交矩阵，满足 $H_n H_n^\top = n I$，其变换可通过快速 Walsh–Hadamard 变换（FWHT）在 $O(n\log n)$ 时间内完成，不涉及乘法，仅需加减法与轻量的缩放。我们对单个量化单元的向量 $x\in\mathbb{R}^n$ 施加标准化后的变换
\[
    \tilde{x} = \frac{1}{\sqrt{n}} H_n x,
\]
然后在 $\tilde{x}$ 空间执行量化。由于 Hadamard 变换实质上把每一维度投影到 $\pm 1$ 组合的正交基上，原本集中于少数维度的能量会被“打散”至各通道，极大地降低单个维度的峰度（kurtosis），从而使可用的量化尺度更贴近大多数数值。推理阶段只需在反量化后施加同样的 Hadamard（其自身就是逆变换），即可还原到原始基底。

典型的 Hadamard 矩阵阶数为 $2^k$。例如，
\[
H_1 = [1],\qquad
H_2 = \begin{bmatrix}
1 & 1 \\
1 & -1
\end{bmatrix},\qquad
H_4 = \begin{bmatrix}
1 & 1 & 1 & 1 \\
1 & -1 & 1 & -1 \\
1 & 1 & -1 & -1 \\
1 & -1 & -1 & 1
\end{bmatrix},
\]
更高阶矩阵可递归构造 $H_{2n} = \begin{bmatrix} H_n & H_n \\ H_n & -H_n \end{bmatrix}$。这些 $\pm1$ 结构使得 FWHT 仅需加减法即可完成正交变换，十分适合硬件实现。

尽管 Hadamard 变换无法完全消除异常值，它显著降低了极端值对量化尺度的牵制，尤其适合配合细粒度 group-wise 方案使用：在每个组内独立执行 FWHT，可在可接受的算力成本下换取更平滑的值分布。但该方法也带来额外的访存与延迟开销，且若组大小与 Hadamard 阶数不匹配，需要通过零填充或子矩阵拼接来适配，这些实现细节将在后续方法部分讨论。

\section{低秩感知的混合精度量化}
从图~\ref{fig:energy_key}和图~\ref{fig:energy_value}中可以观察到，虽然前文提出的算法中的分解目标$S_kW_k$和$\bigl[S_{v,1}W_{v,1},...S_{v,G}W_{v,G}\bigr]$都具有低秩性，即大部分能量都只集中在前几个奇异值上，但是明显能够发现Key的低秩性要更好一些，长尾的奇异值几乎不占多少能量了。因此我们认为对于完全丢弃能量占比最小的秩的做法，对于Key来说损失的信息是可以接受的，而对于Value来说可能造成一些仍然比较有用的信息被丢弃掉了。因此在我们已经通过低秩降维的方式对模型KV缓存进行了压缩后，为了进一步压缩模型，我们将特征维度与量化结合时对Key和Value采用了不同的方法。

对于Key缓存的压缩，我们保留使用~\ref{chap:scaling_svd}和~\ref{chap:rank_search}中秩分配算法得到的低秩裁剪结果。若原始维度为$d$，压缩后每层保留$r_{k,l}$个秩，其空间占用约为原始的一部分，定义低秩压缩率为$\rho_1 = r_{k,l}/d$（或等价的存储体积比）。在该基础上，我们先以16-bit精度存储压缩后的中间值，以保证Key主奇异向量的能量基本无损；随后进一步将这部分数据量化到更低比特，例如8-bit或4-bit，使量化压缩率达到$\rho_2 = 16/b$。最终Key侧的每一层的压缩率可近似写成$\rho_1 \cdot \rho_2$：其中$\rho_1$来自低秩裁剪带来的特征维降维，$\rho_2$来自位宽压缩。若对所有层求平均，则得到模型级别Key缓存的整体压缩率：
\begin{equation}
{\rho}_{\text{key}}=\frac{1}{L}\sum_{l=1}^{L} \rho_{1,l}\rho_{2,l}    
\end{equation}
由于Key的尾部能量极低的奇异值，直接将其舍弃并不会对模型效果有很大影响，因此我们只对原本低秩压缩保留的奇异值和奇异向量计算的Key缓存进行了量化，其他奇异值对应的特征维度则直接丢弃。

在量化前进一步抑制Key缓存中的离群值时，我们借鉴 Palu~\cite{palu} 中的思路，将 Hadamard 变换直接融入到线性投影中。设原始投影矩阵$S_k^{-1}S_kW_k$按低秩分解写作$S_k^{-1}S_kW_k = (S_k^{-1}P_k)Q_k$，我们存储的参数矩阵由原本的$W_k$变为了$S_k^{-1}P_k$和$Q_k$，其中$S_k^{-1}P_k\in\mathbb{R}^{d_{\text{in}}\times r}$、$Q_k\in\mathbb{R}^{r\times d_{\text{out}}}$，推理时缓存的是中间值$XS_k^{-1}P_k$。我们将Hadamard矩阵$H$吸收到前半部分，使得新的前向为$X (S_k^{-1}P_k)' = X (S_k^{-1}P_k H)$；相应地，把$H^{-1}$（在Hadamard情形下等同于$H^\top / r$）折叠进后半部分$Q_k' = H^{-1} Q_k$。如此一来，在线计算得到的中间值$X (S_k^{-1}P_k)'$在产生时就已经过Hadamard变换，天然具备离群值较少的特性，因而可以直接进行量化；反量化之后无需额外施加逆变换，只需与预处理过的$Q_k'$相乘即可还原原始投影效果：
\begin{equation}
    W_k \approx (S_k^{-1}P_k)Q_k = (S_k^{-1}P_k H)(H^\top Q_k) = (S_k^{-1}P_k)'(Q_k)'\label{eq:hadamard_fusion}
\end{equation}
这种“线性层融合变换”的做法不仅省去了在线FWHT的额外成本，也确保了Key缓存量化后仍能保持与未量化版本一致的语义表达，与 Palu~\cite{palu} 的实现保持一致。

我们结合低秩压缩与量化的多角度压缩方式的创新点主要在Value的混合精度压缩上。前文提到，虽然对于Value缓存长尾的奇异值能量较少，提供的信息少，但相比Key缓存直接将低能量奇异值丢弃仍然可能损失一些信息。因此我们综合了量化和低秩降维的思路，采用“保留全部秩、按重要性分配不同数据位宽”的策略：对奇异值较大的重要秩维持较高精度（如8-bit），以保障生成质量；对长尾秩虽然保留，但使用更低比特（如4-bit）来进一步压缩。由于 Value 每层的保留秩 $r_{v,l}$ 不尽相同，为了保持混合精度量化后该层的Value缓存压缩率与“先低秩压缩特征维度，再量化数据类型”的压缩率一致，我们动态调整高精度量化的特征和低精度量化的特征的数量。设第$l$层通过低秩压缩的方式的压缩率为$\rho_1 = r_{v,l}/d$，再量化到更低的$b$比特，量化的压缩率为$\rho_2 = b/16$，那么该层的总压缩率为$\rho_1 \cdot \rho_2$。我们现在需要用“重要秩高精度量化、不重要秩低精度量化”的方式对$l$层达到相同的压缩率，那么就需要对高精度和低精度各自量化的秩的数量进行分配。满秩的特征维度为$d$，高精度数据类型占$b_1$比特，用于量化前$r_1$个秩，低精度数据类型占$b_2$比特，用于量化后$r_2$个秩。为了保持压缩率不变，需要满足：
\begin{align}
    (b\cdot\rho_1 \cdot \rho_2 \cdot d)\text{-bit} & = (b_1 \cdot r_1 + b_2 \cdot r_2)\text{-bit}, \label{eq:equal_bit} \\
    r_1 + r_2 & = d. \label{eq:rank}
\end{align}
其中式~\eqref{eq:equal_bit}表示保持“先低秩再量化”的压缩方式和“低秩性感知的混合精度量化”的压缩方式得到的KV缓存特征维度所占的比特位数相等，式~\eqref{eq:rank}表示高精度量化和低精度量化的秩数量之和等于满秩的情况。

由上述方程组可以解得
\begin{align}
    r_1 &= d \cdot \frac{b \rho_1 \rho_2 - b_2}{b_1 - b_2}, \\
    r_2 &= d \cdot \frac{b_1 - b \rho_1 \rho_2}{b_1 - b_2}.
\end{align}
为保证 $r_1,r_2$ 均为非负整数，我们在实现中会对结果进行四舍五入，并依据组大小或并行约束做少量调节。若 $b_1=8$、$b_2=4$，只要 $b\rho_1\rho_2 \in [b_2, b_1]$ 即可得到合理解；当某层的 $b\rho_1\rho_2$ 过小（靠近纯低比特）时，我们直接令 $r_1=0$，退化为全部低精度量化，这样做在保留全部秩的前提下仍能让压缩率与原方案对齐。

混合精度方案的优势在于：一方面通过高精度保留头部奇异值对应的方向，降低 Value 裁剪带来的生成损失；另一方面维持与原“秩裁剪+统一量化”相当的整体体积，因而不会增加 KV 缓存预算。更重要的是，$r_1$ 与 $r_2$ 的求解完全依赖于上一节得到的秩分配和目标位宽，无需额外的调优环节；我们仅需按层读取奇异值能量排序，确定哪些秩进入高精度集合即可。实践表明，对 Value 使用混合精度量化后，在 Wikitext-2 与 PTB 上的困惑度下降可与 Key 侧低秩+低比特量化相匹配，同时 KV 缓存进一步压缩 10\%～15\%。

\section{可控的混合粒度量化}
前述混合精度框架侧重于“哪些秩用高精度、哪些秩用低精度”。然而在具体实现低比特量化时，我们还必须面对粒度选择带来的性能鸿沟：最初实验结果（表~\ref{tab:value-quant-granularity}）表明，若直接对 Value 施以通道级 2-bit 量化，困惑度会骤然升高；只有切换到更细粒度（如 group-wise）后，才能恢复到可接受水平。这意味着，长尾秩虽然可以以低精度保存，但仍需要细粒度量化来降低量化噪声，否则就会复现粗粒度导致的退化。

问题在于：传统“先低秩再量化”的 pipeline 会导致各层保留的维度 $r_{v,l}$ 互不相同，而 group-wise 量化要求特征数能够整除预设的 group-size（例如 32 或 64），否则组内统计与 dequant 流程都无法对齐。这样一来，要么被迫退回到较粗粒度（牺牲质量），要么在每层为补齐分组而填充零（浪费存储），都不利于 Value 的低比特压缩。

我们在上一节中保留了完整的 $d$ 个秩，并且只通过高/低精度划分来匹配目标压缩率。这一设计带来额外的自由度：对于低精度集合，我们可以按组增减秩（例如增加或减少一个 group 的秩数量），再将多余的秩转移到高精度集合，从而保证低精度部分的特征数始终整除 group-size。具体做法是：先根据式~\eqref{eq:equal_bit}–\eqref{eq:rank} 求得理论的 $r_1,r_2$，随后对 $r_2$ 进行“四舍五入到最近的组数”处理，若发生偏差，则用相同数量从 $r_1$ 中增减秩以保持总数为 $d$ 并维持总 bit 数不变。因为我们仍旧持有所有秩的信息，并未真正截断尾部，所以这种秩调度只改变“哪些秩属于低精度集合”，不会破坏低秩分解本身。

借助这一“可控混合粒度”机制，我们最终实现了三重结合：重要秩使用高精度、无需细粒度；非重要秩使用低精度，并强行对齐到设定的 group-wise 粒度，如此才能真正发挥 2-bit/4-bit 量化的潜力。实验中我们发现，即便将 group-size 设为 32 或 16，依然能够自动适配所有层，无需额外的手工裁剪。因此，本章提出的方案不仅在精度维度上进行了差异化处理，还在粒度上实现了自动可控的对齐，彻底解决了“低秩裁剪导致特征数不整除 group-size”的工程瓶颈。在相同的模型预算与量化位宽下，我们的方案相较于“先低秩再量化”的传统 pipeline 能获得更低的困惑度和更稳定的推理质量，尤其在 Value 分支上表现更为显著；进一步地，我们也将与 Key 相同的 Hadamard 融合策略应用到 Value 的量化分支，以在保持表达一致性的同时减少在线额外变换开销。

\begin{algorithm}[htbp]
\caption{低秩感知的混合精度与可控粒度量化流程}
\label{alg:hybrid-quant}
\KwIn{%
    校准集 $\mathcal{D}_{\text{cal}}$；\\%
    每层奇异值能量 $\{\sigma_{k,l,i}\}, \{\sigma_{v,l,i}\}$；\\%
    目标位宽 $b$、高精度位宽 $b_1$、低精度位宽 $b_2$；\\%
    目标压缩率 $\rho$、group-size $G$；\\%
    Hadamard 阶 $H_l$ 及对应矩阵 $H_l$；\\%
    低秩秩分配结果 $\{r_{k,l}, r_{v,l}\}$，来自~\ref{chap:scaling_svd},~\ref{chap:rank_search}
}
\KwOut{%
    Key 缓存量化参数 $\{s_{k,l}, z_{k,l}, Q_{k,l}\}$；\\%
    Value 缓存混合精度量化参数 $\{s^{(1)}_{v,l}, s^{(2)}_{v,l}, z^{(1)}_{v,l}, z^{(2)}_{v,l}\}$；\\%
    变换后矩阵 $\{(S_k^{-1}P_k)', (Q_k)'\}$ 与 Value 量化掩码
}
\BlankLine
\For{$l \gets 1$ \KwTo $L$}{
    \tcc{Key 分支：低秩 + Hadamard + 单一低比特}
    依据 $r_{k,l}$ 从 $S_k W_k$ 中提取主奇异向量，构造 $S_k^{-1}P_{k,l}$、$Q_{k,l}$；\\
    将 Hadamard 矩阵 $H_l$ 吸入前半部分：$(S_k^{-1}P_{k,l})' \gets S_k^{-1}P_{k,l} H_l$；\\
    将 $H_l^{-1}$ 吸入后半部分：$(Q_{k,l})' \gets H_l^{-1} Q_{k,l}$，如式~\eqref{eq:hadamard_fusion}；\\
    计算低秩压缩率 $\rho_{1,l}^{(k)} = r_{k,l}/d$，位宽压缩率 $\rho_{2,l}^{(k)} = 16/b$；\\
    设定 Key 层压缩率 $\rho_{k,l} = \rho_{1,l}^{(k)} \rho_{2,l}^{(k)}$；\\
    \ForEach{$x \in \mathcal{D}_{\text{cal}}$}{
        计算中间缓存 $h_{k,l} \gets x (S_k^{-1}P_{k,l})'$；\\
        估计对称量化尺度 $s_{k,l}$、零点 $z_{k,l}$ 并量化为 $b$ 比特；\\
        累积 L2 误差以检查量化后精度，如 Section~\ref{sec:dp_for_inference} 所述；
    }
    存储 $\{s_{k,l}, z_{k,l}, (Q_{k,l})'\}$ 作为 Key 层最终参数；
    \BlankLine
    \tcc{Value 分支：混合精度 + 可控粒度}
    计算低秩压缩率 $\rho_{1,l}^{(v)} = r_{v,l}/d$，目标位宽压缩率 $\rho_{2,l}^{(v)} = b/16$；\\
    求解高/低精度秩数 $r_{1,l}, r_{2,l}$，满足式~\eqref{eq:equal_bit} 与~\eqref{eq:rank}；\\
    将 $r_{2,l}$ 四舍五入到最近的 group 数：$\tilde{r}_{2,l} = \operatorname{round}(r_{2,l} / G) \cdot G$；\\
    调整 $r_{1,l} \gets d - \tilde{r}_{2,l}$，并相应更新 $\rho_{2,l}^{(v)}$ 保持等比特约束；\\
    根据奇异值能量排序，选取前 $r_{1,l}$ 个秩为高精度集合 $\mathcal{H}_l$，其余为低精度集合 $\mathcal{L}_l$；\\
    \ForEach{$x \in \mathcal{D}_{\text{cal}}$}{
        计算 Value 缓存 $h_{v,l} \gets x S_{v,l}^{-1} P_{v,l}$；\\
        对 $\mathcal{H}_l$ 使用 $b_1$ 比特量化，得到尺度 $s^{(1)}_{v,l}$、零点 $z^{(1)}_{v,l}$；\\
        将 $\mathcal{L}_l$ 按 group-size $G$ 分组，对每组使用 $b_2$ 比特量化，记录 $s^{(2)}_{v,l,g}$、$z^{(2)}_{v,l,g}$；\\
        若分组后仍出现 outlier，则对该组单独执行 Hadamard 预处理；
    }
    保存高精度与低精度位宽掩码，并记录层级压缩率 $\rho_{v,l}$；
}
\BlankLine
输出平均压缩率 $\rho_{\text{key}} = \frac{1}{L}\sum_l \rho_{k,l}$、$\rho_{\text{value}} = \frac{1}{L}\sum_l \rho_{v,l}$ 以及全部量化参数，供推理阶段直接查表解码；
\end{algorithm}