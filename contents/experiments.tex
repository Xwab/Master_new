\chapter{实验设计与结果分析}
本章面向多个评测维度系统验证所提压缩方案的有效性：我们在语言建模任务（WikiText-2、PTB）上考察困惑度与记忆压缩效果，验证低秩与量化对序列建模基础指标的影响；在长上下文理解领域，我们复现 LongBench 中 21 个异构子任务（覆盖传统一问一答式问答、检索增强推理、事件排序、结构化信息抽取以及多种 Summarization 形式），重点评估模型在 10K～100K token 的长文本输入下是否仍能稳定保持性能；阅读理解与问答方向则涵盖 BoolQ、OpenBookQA、ARC-C、ARC-E 等具有不同推理深度和开放性的问题集合；常识推理任务包括 HellaSwag、Winogrande、PIQA，以考察压缩后的模型在语义完形填空、常识匹配方面的鲁棒性；最后，CEval 提供了中文标准化考试场景，覆盖理工、人文、社科多学科，能综合检验模型跨领域知识与推理能力。在这些任务之上，我们统一报告困惑度、Rouge、准确率、宏 F1 等多种指标，形成一个覆盖生成与判别场景的评测矩阵。

在模型与设置方面，我们选取 Llama3 系列不同容量版本以及 Mixtral 组合专家模型，分别在轻、中、重三档 KV 压缩率（对应不同 rank 预算与量化位宽）下进行评估；上下文长度则覆盖常见的 4K/8K、扩展的 32K，以及极限的 64K/100K，以此观察压缩策略对内存占用与稳定性的影响。为了保证结论严谨，我们首先使用前两章提出的“激活/注意力感知低秩分解 + 层间秩重分配”方案（未启用量化）与当前最强的低秩压缩 baseline 做逐项对比，分析在相同 rank 预算下的性能差距；随后叠加第三章提出的混合精度与可控粒度量化模块，观察在同等压缩率预算下是否进一步降低困惑度、提升生成质量或问答准确率。通过与多种 KV 压缩/量化方法逐一比对，我们展示低秩感知的混合精度策略在复杂推理与长上下文场景中的优势、鲁棒性及可拓展性。