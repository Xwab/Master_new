\chapter{实验设计与结果分析}
本章面向多个评测维度系统验证所提压缩方案的有效性：我们在语言建模任务（WikiText-2、PTB）上考察困惑度与记忆压缩效果，验证低秩与量化对序列建模基础指标的影响；在长上下文理解领域，我们复现 LongBench 中 21 个异构子任务（覆盖传统一问一答式问答、检索增强推理、事件排序、结构化信息抽取以及多种 Summarization 形式），重点评估模型在 10K～100K token 的长文本输入下是否仍能稳定保持性能；阅读理解与问答方向则涵盖 BoolQ、OpenBookQA、ARC-C、ARC-E 等具有不同推理深度和开放性的问题集合；常识推理任务包括 HellaSwag、Winogrande、PIQA，以考察压缩后的模型在语义完形填空、常识匹配方面的鲁棒性；最后，CEval 提供了中文标准化考试场景，覆盖理工、人文、社科多学科，能综合检验模型跨领域知识与推理能力。在这些任务之上，我们统一报告困惑度、Rouge、准确率、宏 F1 等多种指标，形成一个覆盖生成与判别场景的评测矩阵。

在模型与设置方面，我们选取 Llama3 系列不同容量版本以及 Mixtral 组合专家模型，分别在轻、中、重三档 KV 压缩率（对应不同 rank 预算与量化位宽）下进行评估；上下文长度则覆盖常见的 4K/8K、扩展的 32K，以及极限的 64K/100K，以此观察压缩策略对内存占用与稳定性的影响。为了保证结论严谨，我们首先使用前两章提出的“激活/注意力感知低秩分解 + 层间秩重分配”方案（未启用量化）与当前最强的低秩压缩 baseline 做逐项对比，分析在相同 rank 预算下的性能差距；随后叠加第三章提出的混合精度与可控粒度量化模块，观察在同等压缩率预算下是否进一步降低困惑度、提升生成质量或问答准确率。通过与多种 KV 压缩/量化方法逐一比对，我们展示低秩感知的混合精度策略在复杂推理与长上下文场景中的优势、鲁棒性及可拓展性。

\section{实验数据集与评估指标}
\subsection{语言建模与困惑度}
困惑度（Perplexity, PPL）是衡量语言模型预测给定语料能力的经典指标，其本质是序列平均负对数似然的指数形式。设测试集包含 $N$ 个 token，模型对第 $i$ 个 token 的条件概率为 $p_\theta(w_i \mid w_{<i})$，则困惑度定义为
\[
    \mathrm{PPL} = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log p_\theta(w_i \mid w_{<i})\right).
\]
PPL 越低，说明模型越能贴近真实数据分布、准确预测下一个 token。对于 KV 缓存压缩方案而言，困惑度能够最直接地反映压缩带来的语言建模能力损失：若压缩破坏了注意力或隐状态表示，模型对后续 token 的概率分布会偏离，从而显著提升 PPL。为保证对比公平，我们在评估时固定 tokenizer、上下文长度、最大生成步数以及解码温度等因素，确保困惑度差异主要来自压缩策略本身。

在语言建模基准方面，我们选用 WikiText-2 与 Penn Treebank（PTB）两个经典数据集。WikiText-2 来源于维基百科精选条目，保留原始大小写、标点与引用信息，语料规模约 200 万词，句子较长并带有自然的跨句上下文，能够反映真实文本中复杂语法与实体共指等现象。PTB 则基于《华尔街日报》语料，经过严格清洗与词表压缩（约 100 万词），移除了数字与大部分标点，句式更为规整，常用于测试模型在小词表、正式文体下的建模能力。我们分别在这两个数据集上记录各类压缩方案的困惑度，并联合压缩率曲线展现“低秩裁剪→低秩+量化→混合精度”三种方式在基础语言建模任务上的优劣。该实验既能量化语言层面损失，也为后续更复杂任务的表现提供参考上限。

\subsection{零样本推理与准确率}
在零样本（zero-shot）设置下，我们直接使用模型在预训练阶段学到的知识与推理能力，不提供额外监督信号进行微调或少样本提示，而是通过纯粹的 prompt 结构要求模型给出答案。为了全面评估压缩策略对泛化能力的影响，我们选取以下常用的零样本基准，并统一使用准确率（Accuracy）作为评测指标：
\begin{itemize}
    \item \textbf{BoolQ}：是一个面向“是/否”问题的自然问答数据集，共包含 12,697 个样例，其中训练集 9,427 条、验证集 3,270 条。每个样例由（问题，证据段落，布尔答案）组成，问题来自用户在无提示、无约束环境中生成的自然查询，证据段落及可选的页面标题来自 Wikipedia。BoolQ 的文本长度跨度较大（从约 35 token 到 4.7K token），既包含短问题，也涵盖多段长文；其二分类形式与自然语言推断任务类似，评估模型能否依据证据段落对问题做出正确的“是/否”判断。
    \item \textbf{HellaSwag}：常识推理与句子补全数据集，每个样本包含一个上下文及多个候选结尾，模型需要挑选最自然、最合理的续写。难度在于干扰选项设计精巧，需要真实世界知识和语义连贯性。
    \item \textbf{OpenBookQA}：基于开放教科书（OpenBook），提供科学常识和推理问题，每题 4 个选项，需要结合短文知识点与外部常识进行推断，适合检验压缩后模型对科学文本的理解深度。
    \item \textbf{ARC-C / ARC-E}：AI2 Reasoning Challenge 的 Challenge（C）与 Easy（E）子集，均为小学科学考试题目，C 难度更高、干扰项更复杂，主要评估模型在自然语言科学问答上的推理能力。
    \item \textbf{Winogrande}：面向常识的填空式代词消解任务，需要模型理解句中事件关系与指代逻辑，判断空缺处应填哪一个候选词，是衡量常识推理的经典数据集。
    \item \textbf{PIQA}：Physical Interaction QA，聚焦日常物理常识，给定一个情境与两个操作方案，模型需判断哪一个在现实世界更合理，考察物理常识与常规行动逻辑。
    \item \textbf{CEval}：中文综合考试题库，覆盖理工、人文、社科等多个学科，题目形式多为单选题，是衡量中文通用知识与推理能力的重要基准。我们同样在零样本 setting 下使用标准提示格式。
\end{itemize}

在这些数据集上，我们按照官方的 zero-shot prompt 格式，直接让模型输出答案，并统计预测准确率。由于 accuracy 是一个统一的离散指标，能够方便地比较不同压缩策略在多任务零样本推理中的性能差异，也能帮助分析压缩对“无需额外监督即可完成任务”的影响。我们在表格中分别汇报未量化的低秩方案、叠加混合精度量化以及其他 baseline 的准确率，展示在低比特与层间秩分配下模型知识推理能力的保持情况。