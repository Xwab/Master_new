% !TEX root = ../main.tex

\chapter{绪论}



\section{研究背景}
大语言模型(Large Language Model)近几年来发展迅猛，成为自然语言处理领域的核心技术力量。因为其强大的自然语言理解与生成能力，丰富的跨领域知识，能够充分满足用户个性化的需求等优点，大模型已经在智能问答、代码生成、信息检索增强写作、多语种翻译、数据分析助手、教育辅导以及跨模态内容创作等场景中广泛应用，并成为企业知识库、智能客服和自动化研发流程的基础。大预言模型的规模巨大，顶尖的模型参数规模已经达到了数百亿甚至千亿的级别。这种超大规模的参数量使得模型能够捕捉到语言中更加细微的模式和规律，提供更加智能和精准的结果。同时，模型通过海量的无标记数据进行自监督学习，利用大数据集从中学习语言的结构和知识，从而增强了模型的推理和生成能力。大模型还展现出了令人印象深刻的“涌现能力”。在处理某些复杂任务时，能够表现出超出训练数据中显式学习到的能力。例如，当模型达到一定规模时，它可能展现出推理、总结、理解隐含意图等高级语言能力，这些能力并非在训练数据中明确标注过，而是模型通过大规模学习自然语言数据所涌现出来的。然而庞大的规模也为大语言模型的部署和使用带来了诸多挑战。首先，模型的巨大参数量和复杂的计算需求要求极为昂贵的计算资源和存储设备。在训练和推理阶段，大量的存储和计算能力是不可或缺的，这使得大语言模型的部署成本非常高。无论是在科研领域还是商业应用中，为了支持如此巨大的计算负载，通常需要投入大量的资金用于购买高性能的硬件设备，如图形处理单元（GPU）或张量处理单元（TPU）。此外，由于模型的庞大体积，延迟和实时性也成为了实际应用中模型推理时的难题。例如，部署大语言模型进行实时对话或在线服务时，模型的推理速度和响应时间往往会受到计算资源的限制，可能导致用户体验不佳。尤其是在需要多个并发请求时，计算资源的不足可能导致服务器负载过高，影响系统的稳定性和响应效率。为了提高模型的推理效率，基于其以Transformer核心架构的特性和自回归的生成模式，大部分大模型都设计了KV缓存来避免重计算，来减少计算开销，优化推理过程。

大模型的核心架构Transformer通过多头自注意力(Multi-head self-attention)\cite{}机制建模序列中任意位置之间的依赖关系。自注意力机制根据“查询”（Query）和“键”（Key）之间相似度（内积）来决定每个词元对其他词元的关注程度，并使用“值”（Value）来更新每个词元的表示，Query，Key和Value都由输入向量通过模型参数映射得到。每个头负责投影的模型参数不同，用来捕捉不同的语义和句法关系，提高了表达容量，并且多头结构也提高了模型的鲁棒性。传统的Transformer最初采用编码器—解码器(Encoder-Decoder)双路结构，编码器负责编码输入语义，解码器再依据上下文逐步生成输出；而当模型规模扩展至超大参数量、面向按时间顺序逐个生成词元(token)的自回归生成这一核心任务时，模型的结构逐渐转向 decoder-only 架构。Encoder-only架构(如BERT\cite{})和Encoder-Decoder架构(如T5\cite{})，模型通常在推理时一次性处理整个输入序列，不需要逐个词元自回归生成，因此每次推理都需要计算所有词元之间的注意力，也就需要计算所有词元的Query，Key和Value。而Decoder-only架构的自回归生成模式在每生成一个新词元时都需要用到序列中此前所有词元的Key和Value，为了避免每次都对这些值重新计算引起额外的开销，大模型的KV缓存能够在推理时将每个词元的Key和Value存储起来，后续新词元在自注意力层中用到它们时就不需要重新计算了，提高了模型的推理效率。

大规模语言模型在推理阶段依赖 KV 缓存显著减少重复计算、提升生成效率，但这种将所有历史词元的 Key/Value 表征显式保留下来的做法也带来了巨大的显存负担。具体而言，模型的每一层、每一个自注意力头都需要为序列中的每个 token 维护一对 KV 向量，累积起来的缓存体积等于“模型层数 × 头数 × 序列长度 × KV 特征维度 × 存储数据类型大小”。对已经预训练完成的模型而言，层数、头数和特征维度在推理时通常固定不变，因此整体缓存规模几乎完全受序列长度驱动，并以线性方式增长。当输入长度达到上万 token 级别时，即便在单批次推理下，KV 缓存也很容易消耗几十 GB 的显存，使得部署者不得不在上下文长度、批量并发和可用硬件之间做艰难权衡。更棘手的是，庞大的 KV 缓存不仅占据宝贵显存，还持续消耗内存带宽：每生成一步，模型都需要对缓存进行随机访问和写入，访存成为与算力同等重要的性能瓶颈。如果推理系统需要在多设备间传输 KV 缓存（例如流水线并行、张量并行或Prefilling和Decoding分离的场景），跨设备的通信延迟与带宽限制又会成倍放大该成本。由此，KV 缓存随序列长度增长所引发的存储压力和访存开销，已经演化为制约大语言模型推理可扩展性、尤其是长上下文场景下的核心问题之一。

\section{本文研究内容和研究贡献}
\subsection{研究内容}
尽管KV缓存显著降低了大模型自回归推理中的重复计算，但其线性随序列长度增长的存储需求和频繁的访存开销，仍然使长上下文推理面临显存压力、带宽瓶颈等问题。如何在保持模型精度的同时压缩KV缓存的规模以同时节省显存和减少访存开销，已成为优化大模型推理的重要方向。

KV缓存的规模不仅随着序列长度增长而变大，同时也由模型层数，每个词元的KV向量的特征维度大小和其存储在显存中的数据类型决定。因此在考虑大模型KV缓存压缩的时候，可以从上述几个不同的角度入手。从序列长度即模型要处理的词元数量的角度，可以使用滑动窗口只保留最近的词元，或全局地丢弃，合并一些不重要的词元来压缩序列长度。但是这可能影响原本序列的上下文完整性，会牺牲长程依赖，难以兼顾鲁棒性与通用性；从模型不同层的角度，可以通过层间共享KV等方式进行KV缓存的丢弃和复用，来降低整体的层数。然而不同层承担的语义职责差异明显，层级裁剪需要精细调度，否则会使得模型效果大幅下降；从数据类型的角度出发，通过低比特和混合精度量化的方式，直接压缩KV向量每个元素的字节数。这条路线有比较好的兼容性，但是低比特量化对硬件和软件栈的要求较高。

根据实验观察发现，大模型的KV矩阵（词元数量乘以每个词元的KV向量构成的矩阵）的特征维度存在冗余。图%\figure{}
为模型的K矩阵奇异值的累积能量分布曲线。从图中可以看到，在最初几个奇异值处能量累积就急剧上升并迅速接近100\%，说明矩阵的大部分能量（信息）集中在少数几个主成分上，充分表明了其具有低秩的性质，因此可以根据低秩性对特征维度进行压缩。比如使用主成分分析PCA（Principal Component Analysis）或奇异值分解SVD（Singular Value Decomposition）对原本的KV进行降维投影。词元的 KV 缓存本质上由词元隐状态与模型中对应的 KV 投影矩阵相乘得到；因此，如果能够在保持主要语义信息的情况下压缩这一投影矩阵的列空间，就能直接实现特征维度的降维。基于奇异值分解的思路，即将原始的 KV 参数矩阵拆解成两个低维矩阵 (P) 与 (Q) 的乘积，只需在推理时存储词元隐状态与 (P) 相乘得到的低秩表示，再通过 (Q) 重建即可。根据 Eckart–Young–Mirsky 定理 \cite{}，奇异值分解在给定秩约束下提供了最优的二范数近似，因此本文的核心研究之一，就是围绕 SVD 驱动的 KV 缓存低秩降维框架展开。然而，推理过程中 KV 参数矩阵并非孤立存在，它要与激活值、注意力分数等多个张量反复交互。如果直接对整块参数做静态 SVD 并替换原投影，会在激活值和注意力的数值尺度上引入显著偏差，导致模型精度急剧下降。如何解决这一问题，是本文的核心研究内容。

在推进 KV 缓存特征维度压缩时，必须同步考虑大模型各层的职能差异。普遍的认识是：浅层多承担基础词法/句法特征抽取与常识记忆，深层则主导语义理解、跨句关联以及复杂推理。正因如此，各层对维度压缩的敏感性截然不同；浅层的压缩误差还会沿网络层层放大，最终影响深层表示。若仍以统一的压缩率“一刀切”，往往要么对关键层压缩过度、牺牲性能，要么整体压缩率受限、难以显著节省显存。因此，在既定的总体压缩预算下，如何根据层的功能需求、层敏感性或误差累积等指标，动态分配各层的特征维压缩比例，成为本文的另一项核心研究内容。

在面向实际部署的场景中，使用单一角度对KV缓存进行压缩往往存在“压缩比-模型精度”的硬折衷：例如只做特征维度压缩，当压缩比超过某个阈值后，困惑度与下游任务表现会快速劣化。基于此，本文提出进一步研究“KV特征维度压缩 + 其他角度压缩手段”的复合策略，并以压缩KV缓存数据类型，即量化方式为代表进行展开研究。其动机在于：在给定的总体压缩率目标下，将压缩负担分摊到不同维度（特征、数据类型等），可以让每一维都处于相对安全的压缩区间，避免任何单一手段被迫“压到极限”导致性能断崖式下降。同时，特征维压缩聚焦于降低矩阵维度、量化则缩减位宽，二者在数值误差和实现成本上互补，可通过联合优化或分阶段校准最大化保真度。在量化方法中，更细的粒度往往有更好的效果。比如对KV缓存中每一个词元的向量分别量化效果会由于对整个KV缓存矩阵量化。但由于压缩后每层的KV缓存特征维度的不固定，直接使用细粒度量化转换KV缓存数据类型存在困难。因此，本文进一步探索与特征维度自适应压缩协同的分组细粒度量化方案，探索多策略协同压缩的机制与效果，使模型在既定显存预算下保持精度，提升长上下文推理能力。

\subsection{研究贡献}
通过对SVD驱动的KV缓存低秩降维框架，层间特征压缩比例的动态分配和“KV特征维度压缩 + 其他角度压缩手段”的符合KV缓存压缩策略的研究，本文主要做出了以下贡献：
\begin{itemize}

\item 本研究在一般的KV缓存SVD特征维压缩的框架上，显式建模激活值与注意力分数对参数矩阵的作用，构建“激活/注意力–参数乘积”在低秩近似前后的最优重构问题，推导出一套先对参数矩阵进行预变换再分解，最后在重建时通过逆变换还原的步骤，使压缩后的KV缓存既保留原矩阵的主导子空间，保留了原本KV尽可能多的信息，也保证了模型推理时参数矩阵与激活值和注意力相乘的计算结果误差在分解重建前后尽可能小。

\item 本研究利用KV缓存层间低秩性质的差异，结合不同层之间压缩导致误差在后续模型层中传播的误差累积，以及对秩减少的敏感性，提出了创新的层间压缩预算分配策略。该策略以整体压缩率为约束，根据不同层KV的低秩性分配每层的初始压缩率，并根据每层压缩带来的误差累积大小，以及对秩增加和减少的敏感程度，动态调节分配给每层的秩，从而在固定的显存预算下更大程度减少模型效果的劣化。

\item 本研究针对压缩后各层的KV缓存维度不固定，难以引入细粒度量化与特征维度降维结合的问题，设计了可变化的group-wise细粒度量化流程，将量化目标绑定到SVD投影后的局部块，既避免单一手段被迫过度压缩，又让总压缩率在多维组合下分摊误差，最终实现模型推理时相同压缩率下更少的精度下降。

\item 实验部分

\end{itemize}

\section{本文结构介绍}
本文的第一章首先从研究背景，研究内容和贡献的角度介绍了KV缓存在大模型推理时的作用和在长文本推理时其带来的显存占用和访存开销的问题，压缩KV缓存的重要意义以及本研究所提出的基于KV缓存的低秩性进行特征维度压缩的主要贡献，创新性以及解决了该研究内容下的哪些问题。第二章进一步介绍了当前国内外对KV缓存压缩问题的研究情况。第三，四，五章分别介绍了本文的三个主要贡献：以SVD分解为框架，建模激活值，注意力分数对参数矩阵的作用并求解优化问题使得压缩前后“激活/注意力–参数乘积”差异最小；根据不同层的低秩情况，压缩敏感性和压缩后的误差累积在固定KV缓存预算下动态分配每层的压缩率；在压缩后特征维度不固定的场景下，结合细粒度量化压缩KV缓存数据类型和特征维度压缩，使总压缩率在多维组合下分摊误差。第六章详细地给出了实验中我们压缩方法的优秀表现，并和之前的KV缓存特征维度压缩方法进行了对比。在不同压缩率的设定下，分别对不同的开源大模型执行特征维度压缩以及与量化组合压缩的策略，并在困惑度评估、标准指令、代码、Few-shot、QA任务等下游任务以及长文本基准上给出了全面测试结果来证明方法的稳定性和泛化能力。最后第七章则对全文内容进行了总结并提出了下一步研究展望。