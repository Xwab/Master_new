\chapter{层间动态Rank分配与误差调控}
\label{chap:dynamic-rank}

上一章我们已经给出了基于激活/注意力重加权的低秩分解算法，能够在每一层内部找到与真实推理目标 $XW_k$、$A X W_v$ 最匹配的主奇异子空间。本章的方法第二部分在此基础上进一步回答“每一层应该保留多少秩”这一跨层问题：我们不再为所有层设定统一的压缩率，而是同时建模（1）分解目标自身的低秩性，刻画该层在联合 SVD 下主能量的集中程度，并用作 rank 初始化的先验；（2）对丢弃秩的敏感性，评估 Key/Value 在注意力反馈中的梯度放大与残差重建难度，用于指导跨层 rank 传递时的步长；（3）多层串联时低秩近似误差的累积风险，确保深层段的近似误差不会在长链路 KV 缓存里失控，并在不同层之间传递 rank 配额时提供约束信号。通过将这三类指标归一化为层级打分，并在全模型 rank 预算约束下运行一个逐步迭代的贪心分配过程，我们可以为每一层分配自适应的秩上限，保证激活/注意力感知的分解结果有足够的表达度，同时把全局误差保持在推理精度可接受的范围。接下来将从指标构建、分配算法与训练/推理期间的更新策略三个角度展开层间 rank 分配的具体流程。

\section{基于参数低秩性的层间秩初始化}
低秩降维成立的基本前提是：所分解的目标矩阵本身在能量上呈现显著的低秩特性。我们的对象不是原始的 $W_k/W_v$，而是上一章通过变换矩阵 $S_{k,l}$、$S_{v,l}$ 吸收了激活与注意力统计后的 $M_{k,l}=S_{k,l}W_{k,l}$、$M_{v,l}=S_{v,l}W_{v,l}$。只有当这些矩阵在各自的 SVD 中表现出主奇异值快速衰减时，基于奇异值能量的 rank 初始化和裁剪才有意义。虽然 Key 与 Value 普遍呈现低秩特征，但能量集中程度会随层号与矩阵类别发生明显变化：不少层的 Key 在前几个秩内就累计到 80\% 以上，而同层的 Value 曲线往往更平缓，意味着需要保留更多秩才能达到同等能量。累积能量曲线正好把这种差异量化出来，为“哪些层、哪类矩阵可以优先裁剪尾部秩”提供先验。

设模型共有 $L$ 个注意力层，每层在 GQA 架构下含有 $G$ 个 KV 组，每组 Key/Value 维度均为 $d$，一次推理写入缓存的序列数为 $N$，缓存长度为 $T$。在不压缩的情况下，单层写入 KV 缓存的体积为
\begin{equation}
    \label{eq:kv-cost}
    \mathcal{C}_l^{\text{KV}} = 2 N \cdot T \cdot G \cdot d,
\end{equation}
于是模型所有层合计的 KV 体积是 $\mathcal{C}^{\text{KV}} = 2 N T L G d$。由于经过 $S_{k,l}$、$S_{v,l}$ 变换后的 Key/Value 与其 SVD 中间表示在不裁剪秩时的特征维度仍为 $d$，我们可以把 $\mathcal{C}^{\text{KV}}/(N T G)=2Ld$ 视为“单个 token-组对能使用的最大秩数”。若目标压缩率为 $\rho\in(0,1]$，则各层保留的秩必须满足
\[
    \sum_{l=1}^{L} \bigl(r_{k,l}+r_{v,l}\bigr) \le \rho \cdot \frac{\mathcal{C}^{\text{KV}}}{N T G} = \rho \cdot 2 L d.
\]

在这一约束下，初始化的 rank 调度可表述为一个归一化重建误差最小化问题。我们分别对第 $l$ 层的 Key/Value 分解目标保留 $r_{k,l}$、$r_{v,l}$ 个奇异值，最优秩-$r$ 近似记作 $M_{k,l}^{(r_{k,l})}$、$M_{v,l}^{(r_{v,l})}$，需要求解
\begin{equation}
    \label{eq:opt-init}
    \min_{\{r_{k,l}, r_{v,l}\}} \sum_{l=1}^{L}
        \left[
            \frac{\bigl\|M_{k,l} - M_{k,l}^{(r_{k,l})}\bigr\|_F^2}{\|M_{k,l}\|_F^2}
          + \frac{\bigl\|M_{v,l} - M_{v,l}^{(r_{v,l})}\bigr\|_F^2}{\|M_{v,l}\|_F^2}
        \right],
\end{equation}
使得
\begin{equation}
    \label{eq:rank-budget}
    \sum_{l=1}^{L} \Bigl[(d - r_{k,l}) + (d - r_{v,l})\Bigr] = (1-\rho)\cdot 2 L d.
\end{equation}
每一项误差都除以对应矩阵的 $\|M_{k,l}\|_F^2$、$\|M_{v,l}\|_F^2$，这是为了消除不同层能量尺度差异带来的偏置；若直接比较未归一化的能量，能量更大的层即便尾部奇异值占比极小，也会因为绝对值大而被优先保留，无法实现公平的 rank 分配。

\begin{lemma}[F 范数与奇异值能量]\label{lem:frobenius-sigma}
设矩阵 $M$ 的 SVD 为 $M=U\Sigma V^\top$，其中奇异值为 $\{\sigma_i\}$。则
\[
    \|M\|_F^2 = \sum_i \sigma_i^2.
\]
\end{lemma}
该结论可由引理~\ref{lem:Ftotrace}（$\|M\|_F^2=\operatorname{tr}(M^\top M)$）与 $\Sigma^\top \Sigma = \operatorname{diag}(\sigma_i^2)$ 直接推出。

利用 SVD 的最优性，式 \eqref{eq:opt-init} 的误差可以直接写成奇异值能量。记 $\{\sigma_{k,l,i}\}_{i=1}^{d}$、$\{\sigma_{v,l,i}\}_{i=1}^{d}$ 为对应矩阵的降序奇异值，则其相对能量为
\begin{equation}
    \label{eq:energy-weight}
    w_{k,l,i} = \frac{\sigma_{k,l,i}^2}{\sum_{j=1}^{d} \sigma_{k,l,j}^2}, \qquad
    w_{v,l,i} = \frac{\sigma_{v,l,i}^2}{\sum_{j=1}^{d} \sigma_{v,l,j}^2},
\end{equation}
并满足 $\sum_i w_{k,l,i} = \sum_i w_{v,l,i} = 1$。于是
\[
    \frac{\bigl\|M_{k,l} - M_{k,l}^{(r_{k,l})}\bigr\|_F^2}{\|M_{k,l}\|_F^2}
    = \sum_{i=r_{k,l}+1}^{d} w_{k,l,i}, \qquad
    \frac{\bigl\|M_{v,l} - M_{v,l}^{(r_{v,l})}\bigr\|_F^2}{\|M_{v,l}\|_F^2}
    = \sum_{i=r_{v,l}+1}^{d} w_{v,l,i}.
\]
因此，问题 \eqref{eq:opt-init} 等价于在约束 \eqref{eq:rank-budget} 下最大化被保留下来的奇异值能量：
\begin{equation}
    \label{eq:max-energy}
    \max_{\{r_{k,l}, r_{v,l}\}} \sum_{l=1}^{L}
    \left(
        \sum_{i=1}^{r_{k,l}} w_{k,l,i}
      + \sum_{i=1}^{r_{v,l}} w_{v,l,i}
    \right)
    \quad \text{s.t. } \eqref{eq:rank-budget}.
\end{equation}
直观地，这意味着初始化时应优先保留 Key/Value 各自能量占比最高的秩，把能量最小的奇异值视为裁剪候选，直到总共丢弃 $(1-\rho)$ 的秩。

实际实现时无需显式求解 \eqref{eq:max-energy}。我们把所有 $w_{k,l,i}$、$w_{v,l,i}$ 置于同一候选集合（它们已通过各自的 $\|M\|_F^2$ 归一化，可以直接比较），按照能量从小到大依次丢弃奇异值，直至累计删除秩达到 $(1-\rho)\cdot 2Ld$。这等价于对“所有 Key/Value 奇异值能量”做一次全局 top-$\rho$ 选择：能量越大的秩越早被保留，能量最小的秩优先被裁剪。由此得到的 $\{r_{k,l}^{(0)}, r_{v,l}^{(0)}\}$ 既满足全局 rank 预算，又把层间以及 K/V 之间的低秩差异自然映射到初始配额，为下一节的误差传递与敏感性调控迭代提供扎实起点。*** End Patch