\chapter{层间动态Rank分配与误差调控}
\label{chap:dynamic-rank}

上一章我们已经给出了基于激活/注意力重加权的低秩分解算法，能够在每一层内部找到与真实推理目标 $XW_k$、$A X W_v$ 最匹配的主奇异子空间。本章的方法第二部分在此基础上进一步回答“每一层应该保留多少秩”这一跨层问题：我们不再为所有层设定统一的压缩率，而是同时建模（1）分解目标自身的低秩性，刻画该层在联合 SVD 下主能量的集中程度，并用作 rank 初始化的先验；（2）对丢弃秩的敏感性，评估 Key/Value 在注意力反馈中的梯度放大与残差重建难度，用于指导跨层 rank 传递时的步长；（3）多层串联时低秩近似误差的累积风险，确保深层段的近似误差不会在长链路 KV 缓存里失控，并在不同层之间传递 rank 配额时提供约束信号。通过将这三类指标归一化为层级打分，并在全模型 rank 预算约束下运行一个逐步迭代的贪心分配过程，我们可以为每一层分配自适应的秩上限，保证激活/注意力感知的分解结果有足够的表达度，同时把全局误差保持在推理精度可接受的范围。接下来将从指标构建、分配算法与训练/推理期间的更新策略三个角度展开层间 rank 分配的具体流程。

\section{基于参数低秩性的层间秩初始化}
低秩降维成立的基本前提是：所分解的目标矩阵本身在能量上呈现显著的低秩特性。我们的对象不是原始的 $W_k/W_v$，而是上一章通过变换矩阵 $S_{k,l}$、$S_{v,l}$ 吸收了激活与注意力统计后的 $M_{k,l}=S_{k,l}W_{k,l}$、$M_{v,l}=S_{v,l}W_{v,l}$。只有当这些矩阵在各自的 SVD 中表现出主奇异值快速衰减时，基于奇异值能量的 rank 初始化和裁剪才有意义。虽然 Key 与 Value 普遍呈现低秩特征，但能量集中程度会随层号与矩阵类别发生明显变化：不少层的 Key 在前几个秩内就累计到 80\% 以上，而同层的 Value 曲线往往更平缓，意味着需要保留更多秩才能达到同等能量。累积能量曲线正好把这种差异量化出来，为“哪些层、哪类矩阵可以优先裁剪尾部秩”提供先验。

设模型共有 $L$ 个注意力层，每层在 GQA 架构下含有 $G$ 个 KV 组，每组 Key/Value 维度均为 $d$，一次推理写入缓存的序列数为 $N$，缓存长度为 $T$。在不压缩的情况下，单层写入 KV 缓存的体积为
\begin{equation}
    \label{eq:kv-cost}
    \mathcal{C}_l^{\text{KV}} = 2 N \cdot T \cdot G \cdot d,
\end{equation}
于是模型所有层合计的 KV 体积是 $\mathcal{C}^{\text{KV}} = 2 N T L G d$。由于经过 $S_{k,l}$、$S_{v,l}$ 变换后的 Key/Value 与其 SVD 中间表示在不裁剪秩时的特征维度仍为 $d$，我们可以把 $\mathcal{C}^{\text{KV}}/(N T G)=2Ld$ 视为“单个 token-组对能使用的最大秩数”。若目标压缩率为 $\rho\in(0,1]$，则各层保留的秩必须满足
\[
    \sum_{l=1}^{L} \bigl(r_{k,l}+r_{v,l}\bigr) \le \rho \cdot \frac{\mathcal{C}^{\text{KV}}}{N T G} = \rho \cdot 2 L d.
\]

在这一约束下，初始化的 rank 调度可表述为一个归一化重建误差最小化问题。我们分别对第 $l$ 层的 Key/Value 分解目标保留 $r_{k,l}$、$r_{v,l}$ 个奇异值，最优秩-$r$ 近似记作 $M_{k,l}^{(r_{k,l})}$、$M_{v,l}^{(r_{v,l})}$，需要求解
\begin{equation}
    \label{eq:opt-init}
    \min_{\{r_{k,l}, r_{v,l}\}} \sum_{l=1}^{L}
        \left[
            \frac{\bigl\|M_{k,l} - M_{k,l}^{(r_{k,l})}\bigr\|_F^2}{\|M_{k,l}\|_F^2}
          + \frac{\bigl\|M_{v,l} - M_{v,l}^{(r_{v,l})}\bigr\|_F^2}{\|M_{v,l}\|_F^2}
        \right],
\end{equation}
使得
\begin{equation}
    \label{eq:rank-budget}
    \sum_{l=1}^{L} \Bigl[(d - r_{k,l}) + (d - r_{v,l})\Bigr] = (1-\rho)\cdot 2 L d.
\end{equation}
每一项误差都除以对应矩阵的 $\|M_{k,l}\|_F^2$、$\|M_{v,l}\|_F^2$，这是为了消除不同层能量尺度差异带来的偏置；若直接比较未归一化的能量，能量更大的层即便尾部奇异值占比极小，也会因为绝对值大而被优先保留，无法实现公平的 rank 分配。

\begin{lemma}[F 范数与奇异值能量]\label{lem:frobenius-sigma}
设矩阵 $M$ 的 SVD 为 $M=U\Sigma V^\top$，其中奇异值为 $\{\sigma_i\}$。则
\[
    \|M\|_F^2 = \sum_i \sigma_i^2.
\]
\end{lemma}
该结论可由引理~\ref{lem:Ftotrace}（$\|M\|_F^2=\operatorname{tr}(M^\top M)$）与 $\Sigma^\top \Sigma = \operatorname{diag}(\sigma_i^2)$ 直接推出。

利用 SVD 的最优性，式 \eqref{eq:opt-init} 的误差可以直接写成奇异值能量。记 $\{\sigma_{k,l,i}\}_{i=1}^{d}$、$\{\sigma_{v,l,i}\}_{i=1}^{d}$ 为对应矩阵的降序奇异值，则其相对能量为
\begin{equation}
    \label{eq:energy-weight}
    w_{k,l,i} = \frac{\sigma_{k,l,i}^2}{\sum_{j=1}^{d} \sigma_{k,l,j}^2}, \qquad
    w_{v,l,i} = \frac{\sigma_{v,l,i}^2}{\sum_{j=1}^{d} \sigma_{v,l,j}^2},
\end{equation}
并满足 $\sum_i w_{k,l,i} = \sum_i w_{v,l,i} = 1$。于是
\[
    \frac{\bigl\|M_{k,l} - M_{k,l}^{(r_{k,l})}\bigr\|_F^2}{\|M_{k,l}\|_F^2}
    = \sum_{i=r_{k,l}+1}^{d} w_{k,l,i}, \qquad
    \frac{\bigl\|M_{v,l} - M_{v,l}^{(r_{v,l})}\bigr\|_F^2}{\|M_{v,l}\|_F^2}
    = \sum_{i=r_{v,l}+1}^{d} w_{v,l,i}.
\]
因此，问题 \eqref{eq:opt-init} 等价于在约束 \eqref{eq:rank-budget} 下最大化被保留下来的奇异值能量：
\begin{equation}
    \label{eq:max-energy}
    \max_{\{r_{k,l}, r_{v,l}\}} \sum_{l=1}^{L}
    \left(
        \sum_{i=1}^{r_{k,l}} w_{k,l,i}
      + \sum_{i=1}^{r_{v,l}} w_{v,l,i}
    \right)
    \quad \text{s.t. } \eqref{eq:rank-budget}.
\end{equation}
直观地，这意味着初始化时应优先保留 Key/Value 各自能量占比最高的秩，把能量最小的奇异值视为裁剪候选，直到总共丢弃 $(1-\rho)$ 的秩。

实际实现时无需显式求解 \eqref{eq:max-energy}。我们把所有 $w_{k,l,i}$、$w_{v,l,i}$ 置于同一候选集合（它们已通过各自的 $\|M\|_F^2$ 归一化，可以直接比较），按照能量从小到大依次丢弃奇异值，直至累计删除秩达到 $(1-\rho)\cdot 2Ld$。这等价于对“所有 Key/Value 奇异值能量”做一次全局 top-$\rho$ 选择：能量越大的秩越早被保留，能量最小的秩优先被裁剪。由此得到的 $\{r_{k,l}^{(0)}, r_{v,l}^{(0)}\}$ 既满足全局 rank 预算，又把层间以及 K/V 之间的低秩差异自然映射到初始配额，为下一节的误差传递与敏感性调控迭代提供扎实起点。

\section{基于误差累积与敏感性的层间秩重分配}
初始化得到的 $\{r_{k,l}^{(0)}, r_{v,l}^{(0)}\}$ 默认为各层误差彼此独立。为了显式建模压缩误差在深层网络中的传递，我们在一小部分校准集 $\mathcal{D}_\text{cal}$ 上对“压缩 vs.~不压缩”的输出差异进行观测，把误差累积和秩敏感性纳入贪心重分配流程。

\subsection{校准集驱动的误差观测}
对于每个样本 $x\in\mathcal{D}_\text{cal}$，我们逐层运行在当前层间秩分配情况下的压缩后的模型，并记录每一层的输出来作为下一层的输入（最后一层的输出即为模型的最终输出）。当样本前向到 $l$ 层时，我们对 $l$ 层复制两次，记为 $l_k$ 层和 $l_v$ 层。对于 $l_k$ 层我们保持 $\bigl[S_{v,1}W_{v,1},...,S_{v,G}W_{v,G}\bigr]$ 分配到的秩不变，而对于 $S_kW_k$ 则是还原回原本的 $d$ 个秩，即 $l_k$ 层是 $l$ 层在当前模型秩分配下的只压缩 Value 不压缩 Key 的版本。类似地，我们对 $l_v$ 层保持 $S_kW_k$ 分配的秩不变，将 $\bigl[S_{v,1}W_{v,1},...,S_{v,G}W_{v,G}\bigr]$ 分配的秩还原回 $d$，就得到了 $l$ 层只压缩 Key 不压缩 Value 的版本。

对于每一层都进行这样的操作，如果模型总共有 $L$ 层，则相当于得到了 $2L+1$ 个不同的模型，其中 1 表示当前秩分配下的原本模型，而 $2L$ 表示每一层的 Key/Value 分别不进行压缩，其他层的所有 Key 和 Value 以及当前层的 Value/Key 都保留当前秩分配时候的模型。我们将得到的这 $2L$ 个模型的输出与原本秩分配下模型的输出进行对比，计算 L2 范数。L2 范数越大说明压缩该层对模型输出的影响越大，造成的误差累积以及压缩该层本身的影响越大，因此需要给这样的层分配更多的秩。反之则可以分配更少的秩，并将多出来的秩分配给其他层。但如果对每个样本都用 $2L+1$ 个模型去进行前向得到 $2L+1$ 个输出速度会很慢，为了解决这个问题本文提出了一个动态规划的思路在达到同样目的的情况下提升效率。

记第 $l$ 层为 $F_{l}()$，$l_k$ 层和 $l_v$ 层分别为 $F_{l,k}()$ 和 $F_{l,v}()$，第 $l$ 层的输出为 $h_l$，$l_k$ 层和 $l_v$ 层的输出分别为 $h_{l,k}$ 和 $h_{l,v}$。动态规划算法具体做法是：维护一个队列 $\mathcal{Q}$，以第 0 层为例，当样本 $x$ 输入时，分别记录 $F_{0}(x)=h_0$、$F_{0,k}(x)=h_{0,k}$、$F_{0,v}(x)=h_{0,v}$，并将 $h_{0,k}$ 和 $h_{0,v}$ 输入队列 $\mathcal{Q}$ 中。当前向推理进行到第 1 层时，计算压缩了 KV 的输出 $F_{1}(h_0)=h_1$，并将队列中的 $h_{0,k}$ 和 $h_{0,v}$ 依次从队首出队，每有一个出队的元素，就将出队的元素作为输入送入 1 层的 $F_{1}()$ 中，最后计算得到 $F_{1}(h_{0,k})$，$F_{1}(h_{0,v})$，将其作为新的 $h_{0,k}$ 和 $h_{0,v}$ 再从队列尾部入队。除了队列中的元素外，还将上一层的输出 $h_0$ 输入给当前层（即第 1 层）K 和 V 分别不压缩的变体，得到 $h_{1,k}=F_{1,k}(h_0)$，$h_{1,v}=F_{1,v}(h_0)$，并将它们在 $h_{0,k}$ 和 $h_{0,v}$ 之后入队。一般地，对于前向推理到第 $l$ 层时，先计算压缩 KV 的 $l$ 层的输出 $h_l=F_{l}(h_{l-1})$；此时队列中的元素为 $\bigl[ h_{0,k},h_{0,v},...,h_{l-1,k},h_{l-1,v}\bigr]$，将各组元素（同一层的 KV 看作一组）依次从队首出队，第 $i$ 组元素($0 \le i \le l-1$)为 $h_{i,k}$ 和 $h_{i,v}$，计算 $F_{l}(h_{i,k})$ 和 $F_{l}(h_{i,v})$ 作为新的 $h_{i,k}$ 和 $h_{i,v}$ 再从队尾入队，直到队列中原本的 $l$ 组元素都经过了这样出队->计算->入队的过程。其代表的含义是第 $i$ 层的 K/V 不压缩时，其输出经过第 $l$ 层之后的输出；最后还需要计算第 $l$ 层的 K/V 不压缩时的 $h_{l,k}=F_{l,k}(h_{l-1})$ 和 $h_{l,v}=F_{l,v}(h_{l-1})$，并放入队列末尾。当该过程进行到最后一层时，队列中会有 $2L$ 个元素，分别代表模型各个层的 K/V 不压缩，其他层的 K/V 和当前层的 V/K 按照当前秩分配进行压缩的情况下，模型最终的输出。再加上所有层的 KV 都按照当前的秩分配进行压缩得到的输出，我们一共会得到 $2L+1$ 个输出。这个算法避免了用 $2L+1$ 个不同模型都进行一次前向推理带来的更多耗时，得到了与其同样的效果。算法的伪代码如下：

\begin{center}
\begin{minipage}{0.95\linewidth}
\textbf{算法 1：动态规划式单层解压前向}

\textbf{Input:} 样本 $x$，层数 $L$，映射 $F_l,F_{l,k},F_{l,v}$ \
\textbf{Output:} $y^{\text{full}}$ 及 $\{y^{(k,l)},y^{(v,l)}\}_{l=0}^{L-1}$

\begin{enumerate}
    \item 初始化队列 $\mathcal{Q}$；置 $h_0=F_0(x)$，$h_{0,k}=F_{0,k}(x)$，$h_{0,v}=F_{0,v}(x)$ 并入队。
    \item \textbf{for} $l=1$ \textbf{to} $L-1$ \textbf{do}
    \begin{enumerate}
        \item $h_l \gets F_l(h_{l-1})$。
        \item \textbf{for each} $(h_{i,k},h_{i,v})\in\mathcal{Q}$ \textbf{do}\\
        \quad $h_{i,k} \gets F_l(h_{i,k})$，$h_{i,v} \gets F_l(h_{i,v})$，并将更新后的对重新入队。
        \item $h_{l,k}\gets F_{l,k}(h_{l-1})$，$h_{l,v}\gets F_{l,v}(h_{l-1})$，将 $(h_{l,k},h_{l,v})$ 入队。
    \end{enumerate}
    \item 返回 $y^{\text{full}}=h_{L-1}$ 以及队列中每对元素经末层映射后的 $\{y^{(k,l)},y^{(v,l)}\}$。
\end{enumerate}
\end{minipage}
\end{center}

\subsection{误差驱动与敏感性调控的贪心传递}
在当前秩配置下定义平均敏感性
\begin{equation}
    g_{k,l}=\frac{\overline{\Delta}_{k,l}}{d-r_{k,l}+\epsilon},\qquad
    g_{v,l}=\frac{\overline{\Delta}_{v,l}}{d-r_{v,l}+\epsilon},
\end{equation}
其中 $\epsilon$ 为极小常数，用于避免分母为零。$g$ 描述了“单位秩变化”在该层上带来的平均 L2 误差，越大说明该层对 rank 调整越敏感。令 $\overline{\Delta}$ 为所有层的平均误差，使用
\[
    \alpha_{k,l}=\frac{d/\overline{\Delta}}{g_{k,l}},\qquad
    \alpha_{v,l}=\frac{d/\overline{\Delta}}{g_{v,l}}
\]
对敏感性进行归一化，便于跨层比较。

每轮迭代中，我们选取误差最大的层 $\ell_\text{max}$ 与误差最小、且仍有冗余秩可回收的层 $\ell_\text{min}$，设置步长
\[
    \delta = \eta\bigl(\alpha_{\ell_\text{max}}+\alpha_{\ell_\text{min}}\bigr),
\]
其中 $\eta$ 为学习率。通过 $r_{\ell_\text{max}}\leftarrow r_{\ell_\text{max}}+\delta$、$r_{\ell_\text{min}}\leftarrow r_{\ell_\text{min}}-\delta$ 完成一次 rank 传递，并在新的秩配置下重新运行校准。为了避免震荡，我们在实践中还会设置最小/最大步长、允许多层并行回收（例如一次从误差排名最靠前的三层补给、从误差排名最低的若干层回收），以及引入指数滑动平均来平滑 $\text{err}$ 序列；若连续 $K$ 轮迭代都未降低“压缩模型 vs.~full 模型”的 L2 误差，则回滚至最佳秩配置并终止。

\subsection{实现细节与复杂度分析}
该贪心策略的开销主要来自两部分：校准前向与秩重分配计算。若校准集包含 $|\mathcal{D}_\text{cal}|$ 个样本，单轮评估的额外时间大约为一次 full-rank 推理（用于缓存）加一次压缩推理；我们在实验中通常选择 $|\mathcal{D}_\text{cal}|=128\sim256$，并可在不同指令集或不同输入分布上重复采样，以防止某一类样本“绑架”秩分配。秩更新的复杂度接近 $O(L)$，因为我们只需扫描所有层的误差与敏感性即可确定 $\ell_\text{max}$、$\ell_\text{min}$。在多核环境中，校准运行与秩评估也可以管线化：一批层在进行重分配的同时，另一批样本的 full-rank 输出已在后台预计算。

此外，算法 1 中的 “Repeat” 循环可根据应用场景选择不同的收敛准则：\textbf{(i)} 当全局 L2 误差降幅低于某阈值 $\tau$ 时停止；\textbf{(ii)} 当所有层的敏感性 $g_l$ 落在预设区间内（表示无明显“短板”层）时停止；\textbf{(iii)} 当 rank 传递已经导致若干层触及上/下限，需要重新规划预算时停止。这些附加规则有助于在实际系统中控制动态调度对时延的影响。

\subsection{伪代码}
下列伪代码总结了整个 rank 重分配过程，$r_l$ 可指代 Key 或 Value 的秩向量。

\begin{center}
\begin{minipage}{0.95\linewidth}
\textbf{算法 1：误差-敏感性联合驱动的秩重分配}
\begin{enumerate}
    \item \textbf{初始化：} 根据上一节的 $\{r_l^{(0)}\}$，在 $\mathcal{D}_\text{cal}$ 上运行一次校准，得到每层的 $\text{err}_l=\overline{\Delta}_l$ 和敏感性 $g_l=\text{err}_l/(d-r_l+\epsilon)$。
    \item \textbf{Repeat}
    \begin{enumerate}
        \item $\ell_\text{max}=\arg\max_l \text{err}_l$，$\ell_\text{min}=\arg\min_l \text{err}_l$。
        \item 计算 $\alpha_l=(d/\overline{\text{err}})/g_l$，其中 $\overline{\text{err}}$ 为当前所有层误差的均值。
        \item 设 $\delta=\eta(\alpha_{\ell_\text{max}}+\alpha_{\ell_\text{min}})$，更新 $r_{\ell_\text{max}}\leftarrow r_{\ell_\text{max}}+\delta$、$r_{\ell_\text{min}}\leftarrow r_{\ell_\text{min}}-\delta$，并裁剪到 $0<r_l\le d$。
        \item 重新运行校准得到新的 $\text{err}_l$、$g_l$；若全局 L2 误差未减小或迭代次数超过 $K$，则恢复上一轮秩配置并终止。
    \end{enumerate}
    \item \textbf{Until} 收敛或达到迭代上限。
\end{enumerate}
\end{minipage}
\end{center}

依托“误差观测 + 敏感性调控 + 固定预算”三重约束，该贪心迭代可以在总体秩不变的前提下，把额外的表达能力逐步让渡给误差最大的层，并抑制深层链路中的误差积累。

\subsection{动态规划式输出缓存伪代码}
为便于复现实验，以下伪代码展示了如何用单次校准前向获得 $2L+1$ 组输出。记 $F_l$ 为第 $l$ 层压缩后的映射，$F_{l,k}$、$F_{l,v}$ 为“仅第 $l$ 层 Key/Value 不压缩”的映射，队列 $\mathcal{Q}$ 维护着各层“解除压缩”分支在当前层的输入。

\begin{center}
\begin{minipage}{0.95\linewidth}
\textbf{算法 2：单次校准生成 $2L+1$ 组输出}

\textbf{Input:} 样本 $x$，层数 $L$，映射 $F_l,F_{l,k},F_{l,v}$ \\
\textbf{Output:} $\bigl\{y^{\text{full}}, y^{(k,l)}, y^{(v,l)}\bigr\}_{l=0}^{L-1}$

\begin{enumerate}
    \item $\mathcal{Q}\gets\varnothing$；$h_0 \gets F_0(x)$；$h_{0,k}\gets F_{0,k}(x)$，$h_{0,v}\gets F_{0,v}(x)$；将 $h_{0,k},h_{0,v}$ 入队。
    \item \textbf{for} $l=1$ \textbf{to} $L-1$ \textbf{do}
    \begin{enumerate}
        \item $h_l \gets F_l(h_{l-1})$。
        \item \textbf{for each} $(h_{i,k},h_{i,v})$ \textbf{in} $\mathcal{Q}$（按入队顺序）\textbf{do}\\
        \quad $h_{i,k} \gets F_l(h_{i,k})$，$h_{i,v}\gets F_l(h_{i,v})$，并将更新后的 $(h_{i,k},h_{i,v})$ 重新入队。
        \item 计算当前层的未压缩分支：$h_{l,k}\gets F_{l,k}(h_{l-1})$，$h_{l,v}\gets F_{l,v}(h_{l-1})$，并入队 $(h_{l,k},h_{l,v})$。
    \end{enumerate}
    \item 最终输出：$y^{\text{full}} = h_{L-1}$；队列中每对 $(h_{i,k},h_{i,v})$ 经过末层映射即可得到 $y^{(k,i)}, y^{(v,i)}$。
\end{enumerate}
\end{minipage}
\end{center}

该流程等价于显式构造 $2L+1$ 个不同的模型并逐一前向，但仅需一次 full-rank/压缩共享的管道，因而能在保证统计精度的同时显著降低校准成本。

