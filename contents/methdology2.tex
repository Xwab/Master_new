\chapter{层间动态Rank分配与误差调控}
\label{chap:dynamic-rank}

上一章我们已经给出了基于激活/注意力重加权的低秩分解算法，能够在每一层内部找到与真实推理目标 $XW_k$、$A X W_v$ 最匹配的主奇异子空间。本章的方法第二部分在此基础上进一步回答“每一层应该保留多少秩”这一跨层问题：我们不再为所有层设定统一的压缩率，而是同时建模（1）分解目标自身的低秩性，刻画该层在联合 SVD 下主能量的集中程度，并用作 rank 初始化的先验；（2）对丢弃秩的敏感性，评估 Key/Value 在注意力反馈中的梯度放大与残差重建难度，用于指导跨层 rank 传递时的步长；（3）多层串联时低秩近似误差的累积风险，确保深层段的近似误差不会在长链路 KV 缓存里失控，并在不同层之间传递 rank 配额时提供约束信号。通过将这三类指标归一化为层级打分，并在全模型 rank 预算约束下运行一个逐步迭代的贪心分配过程，我们可以为每一层分配自适应的秩上限，保证激活/注意力感知的分解结果有足够的表达度，同时把全局误差保持在推理精度可接受的范围。接下来将从指标构建、分配算法与训练/推理期间的更新策略三个角度展开层间 rank 分配的具体流程。

\section{基于参数低秩性的层间秩初始化}
层间动态分配的第一步是厘清“每一层原本需要多少秩”。设模型共有 $L$ 个注意力层，第 $l$ 层在 GQA 架构下含有 $G_l$ 个 KV 组、每组 Key/Value 维度分别为 $d_k^{(l)}$、$d_v^{(l)}$，推理缓存长度为 $T$。不施加压缩时，该层需要写入的 KV 存储量为
\begin{equation}
    \label{eq:kv-cost}
    \mathcal{C}_l^{\text{KV}} = T \cdot G_l \cdot \bigl(d_k^{(l)} + d_v^{(l)}\bigr),
\end{equation}
对应的可用 rank 上限也是 $d_k^{(l)} + d_v^{(l)}$。为了在总压缩率 $\rho \in (0,1]$ 约束下初始化各层秩，我们先将基于上一章得到的“激活/注意力加权”矩阵
\[
M_{k,l} = S_{k,l} W_{k,l}, \qquad
M_{v,l} = S_{v,l} W_{v,l}
\]
做联合 SVD，记其奇异值序列为 $\{\sigma_{l,i}\}_{i=1}^{d_l}$，其中 $d_l = d_k^{(l)} + d_v^{(l)}$。每一个奇异值对应一维 rank，能量权重定义为
\begin{equation}
    \label{eq:energy-weight}
    w_{l,i} = \frac{\sigma_{l,i}^2}{\sum_{j=1}^{d_l} \sigma_{l,j}^2}, \qquad
    \sum_{i=1}^{d_l} w_{l,i} = 1.
\end{equation}
该相对能量直接衡量了第 $i$ 个秩对重构误差的贡献：$w_{l,i}$ 越大，说明舍弃该奇异向量会丢失越多能量，因而越不应当被压缩。

我们将全部层的能量权重按照“自顶向下保留能量”方式来初始化秩：首先依据每层原始 KV 成本分配一个目标保留能量份额
\begin{equation}
    \label{eq:energy-budget}
    \beta_l = \frac{\mathcal{C}_l^{\text{KV}}}{\sum_{j=1}^{L} \mathcal{C}_j^{\text{KV}}}, \qquad
    \mathcal{E}_l^{\text{keep}} = \rho \cdot \beta_l,
\end{equation}
其中 $\rho$ 是全模型希望保留的 rank/存储占比（$\rho = 0.4$ 即只保留 40\% 的 rank 能量）。然后对第 $l$ 层的奇异值按照能量权重从大到小排序，找到最小的秩 $r_l^{(0)}$ 使得
\begin{equation}
    \label{eq:init-rank}
    \sum_{i=1}^{r_l^{(0)}} w_{l,i} \ge \mathcal{E}_l^{\text{keep}},
\end{equation}
并将该 $r_l^{(0)}$ 作为层 $l$ 的初始秩上限。换言之，我们优先保留能量占比最大的奇异向量，把能量占比最小的那部分（$\sum_{i=r_l^{(0)}+1}^{d_l} w_{l,i}$）视为“可安全丢弃”的秩，直到累计丢弃量达到 $(1-\rho)$ 对应的总能量。

上述流程等价于一个跨层的“能量截断”操作：所有层共享统一的压缩率 $\rho$，但由于 $\beta_l$ 受到原始 KV 成本的加权，KV 更昂贵的层会分到更高的保留能量，从而初始秩也更大。该初始化结果不仅满足 $\sum_l r_l^{(0)} \approx \rho \sum_l d_l$ 的全局预算，还为后续“误差累积约束 + 敏感性控制步长”的迭代贪心提供了一个有物理意义的起点。接下来我们将在下一节引入误差传递与敏感性信息，对 $r_l^{(0)}$ 进行细化与动态调整。