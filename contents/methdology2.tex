\chapter{层间动态Rank分配与误差调控}
\label{chap:dynamic-rank}

上一章我们已经给出了基于激活/注意力重加权的低秩分解算法，能够在每一层内部找到与真实推理目标 $XW_k$、$A X W_v$ 最匹配的主奇异子空间。本章的方法第二部分在此基础上进一步回答“每一层应该保留多少秩”这一跨层问题：我们不再为所有层设定统一的压缩率，而是同时建模（1）分解目标自身的低秩性，刻画该层在联合 SVD 下主能量的集中程度，并用作 rank 初始化的先验；（2）对丢弃秩的敏感性，评估 Key/Value 在注意力反馈中的梯度放大与残差重建难度，用于指导跨层 rank 传递时的步长；（3）多层串联时低秩近似误差的累积风险，确保深层段的近似误差不会在长链路 KV 缓存里失控，并在不同层之间传递 rank 配额时提供约束信号。通过将这三类指标归一化为层级打分，并在全模型 rank 预算约束下运行一个逐步迭代的贪心分配过程，我们可以为每一层分配自适应的秩上限，保证激活/注意力感知的分解结果有足够的表达度，同时把全局误差保持在推理精度可接受的范围。接下来将从指标构建、分配算法与训练/推理期间的更新策略三个角度展开层间 rank 分配的具体流程。

\section{基于参数低秩性的层间秩初始化}
低秩降维成立的基本前提是：所分解的目标矩阵本身在能量上呈现显著的低秩特性。我们的对象不是原始的 $W_k/W_v$，而是上一章通过变换矩阵 $S_{k,l}$、$S_{v,l}$ 吸收了激活与注意力统计后的 $M_{k,l}=S_{k,l}W_{k,l}$、$M_{v,l}=S_{v,l}W_{v,l}$。只有当这些矩阵在各自的 SVD 中表现出主奇异值快速衰减时，基于奇异值能量的 rank 初始化和裁剪才有意义。虽然 Key 与 Value 普遍呈现低秩特征，但能量集中程度会随层号与矩阵类别发生明显变化：不少层的 Key 在前几个秩内就累计到 80\% 以上，而同层的 Value 曲线往往更平缓，意味着需要保留更多秩才能达到同等能量。累积能量曲线正好把这种差异量化出来，为“哪些层、哪类矩阵可以优先裁剪尾部秩”提供先验。

设模型共有 $L$ 个注意力层，每层在 GQA 架构下含有 $G$ 个 KV 组，每组 Key/Value 维度均为 $d$，一次推理写入缓存的序列数为 $N$，缓存长度为 $T$。在不压缩的情况下，单层写入 KV 缓存的体积为
\begin{equation}
    \label{eq:kv-cost}
    \mathcal{C}_l^{\text{KV}} = 2 N \cdot T \cdot G \cdot d,
\end{equation}
于是模型所有层合计的 KV 体积是 $\mathcal{C}^{\text{KV}} = 2 N T L G d$。由于经过 $S_{k,l}$、$S_{v,l}$ 变换后的 Key/Value 与其 SVD 中间表示在不裁剪秩时的特征维度仍为 $d$，我们可以把 $\mathcal{C}^{\text{KV}}/(N T G)=2Ld$ 视为“单个 token-组对能使用的最大秩数”。若目标压缩率为 $\rho\in(0,1]$，则各层保留的秩必须满足
\[
    \sum_{l=1}^{L} r_l \le \rho \cdot \frac{\mathcal{C}^{\text{KV}}}{N T G} = \rho \cdot 2 L d.
\]

在这一约束下，初始化的 rank 调度可表述为一个归一化重建误差最小化问题。记 $M_l=[M_{k,l},M_{v,l}] \in \mathbb{R}^{d\times 2d}$ 为第 $l$ 层 Key/Value 目标的拼接矩阵，$M_l^{(r_l)}$ 是其最优秩-$r_l$ 近似。我们希望求解
\begin{equation}
    \label{eq:opt-init}
    \min_{\{r_l\}} \sum_{l=1}^{L}
        \frac{\bigl\|M_l - M_l^{(r_l)}\bigr\|_F^2}{\|M_l\|_F^2},
\end{equation}
使得
\begin{equation}
    \label{eq:rank-budget}
    \sum_{l=1}^{L} (2d - r_l) = (1-\rho)\cdot 2 L d.
\end{equation}
每一项误差都除以 $\|M_l\|_F^2$，是为了抵消不同层能量尺度差异带来的偏置；如果省略归一化，能量较大的层即便尾部奇异值占比很小，也会因为绝对值大而被优先保留，无法实现公平的 rank 分配。

利用 SVD 的最优性，式 \eqref{eq:opt-init} 中的误差可直接换成奇异值能量。记 $\{\sigma_{l,i}\}_{i=1}^{2d}$ 为 $M_l$ 的降序奇异值，则
\begin{equation}
    \label{eq:energy-weight}
    w_{l,i} = \frac{\sigma_{l,i}^2}{\sum_{j=1}^{2d} \sigma_{l,j}^2}, \qquad
    \sum_{i=1}^{2d} w_{l,i} = 1,
\end{equation}
并且
\[
    \frac{\bigl\|M_l - M_l^{(r_l)}\bigr\|_F^2}{\|M_l\|_F^2}
    = \sum_{i=r_l+1}^{2d} w_{l,i}.
\]
因此，问题 \eqref{eq:opt-init} 等价于在约束 \eqref{eq:rank-budget} 下最大化被保留下来的奇异值能量：
\begin{equation}
    \label{eq:max-energy}
    \max_{\{r_l\}} \sum_{l=1}^{L} \sum_{i=1}^{r_l} w_{l,i}
    \quad \text{s.t. } \eqref{eq:rank-budget}.
\end{equation}
直观地，这意味着初始化时应优先保留所有层（Key/Value 拼接后）中能量占比最高的秩，把能量最小的奇异值视为裁剪候选，直到总共丢弃 $(1-\rho)$ 的秩。

实际实现时无需显式求解 \eqref{eq:max-energy}。我们只需对每层的 $\{w_{l,i}\}$ 按降序排序，然后从尾部开始删除奇异值，累积删除量达到 $(1-\rho)\cdot 2Ld$ 即可。这等价于对“所有奇异值能量”做一次全局 top-$\rho$ 选择，既满足全局 rank 预算，又把不同层、不同类型矩阵的低秩差异自然地反映到初始秩上。下一节将在此基础上进一步考虑层间误差传递约束，对这些初值做细化。*** End Patch