\chapter{层间动态Rank分配与误差调控}
\label{chap:dynamic-rank}

上一章我们已经给出了基于激活/注意力重加权的低秩分解算法，能够在每一层内部找到与真实推理目标 $XW_k$、$A X W_v$ 最匹配的主奇异子空间。本章的方法第二部分在此基础上进一步回答“每一层应该保留多少秩”这一跨层问题：我们不再为所有层设定统一的压缩率，而是同时建模（1）分解目标自身的低秩性，刻画该层在联合 SVD 下主能量的集中程度，并用作 rank 初始化的先验；（2）对丢弃秩的敏感性，评估 Key/Value 在注意力反馈中的梯度放大与残差重建难度，用于指导跨层 rank 传递时的步长；（3）多层串联时低秩近似误差的累积风险，确保深层段的近似误差不会在长链路 KV 缓存里失控，并在不同层之间传递 rank 配额时提供约束信号。通过将这三类指标归一化为层级打分，并在全模型 rank 预算约束下运行一个逐步迭代的贪心分配过程，我们可以为每一层分配自适应的秩上限，保证激活/注意力感知的分解结果有足够的表达度，同时把全局误差保持在推理精度可接受的范围。接下来将从指标构建、分配算法与训练/推理期间的更新策略三个角度展开层间 rank 分配的具体流程。

\section{基于参数低秩性的层间秩初始化}
低秩降维成立的基本前提是：所分解的目标矩阵本身在能量上呈现显著的低秩特性。我们的对象不是原始的 $W_k/W_v$，而是上一章通过变换矩阵 $S_{k,l}$、$S_{v,l}$ 吸收了激活与注意力统计后的 $M_{k,l}=S_{k,l}W_{k,l}$、$M_{v,l}=S_{v,l}W_{v,l}$。只有当这些矩阵在各自的 SVD 中表现出主奇异值快速衰减时，基于奇异值能量的 rank 初始化和裁剪才有意义。虽然 Key 与 Value 普遍呈现低秩特征，但能量集中程度会随层号与矩阵类别发生明显变化：不少层的 Key 在前几个秩内就累计到 80\% 以上，而同层的 Value 曲线往往更平缓，意味着需要保留更多秩才能达到同等能量。累积能量曲线正好把这种差异量化出来，为“哪些层、哪类矩阵可以优先裁剪尾部秩”提供先验。

跨层 rank 调度的起点是弄清楚各层原始的 KV 规模。设模型共有 $L$ 个注意力层，第 $l$ 层在 GQA 架构下含有 $G_l$ 个 KV 组，每组 Key/Value 维度分别为 $d_k^{(l)}$、$d_v^{(l)}$，推理缓存长度为 $T$。不压缩时写入 KV 缓存的体积是
\begin{equation}
    \label{eq:kv-cost}
    \mathcal{C}_l^{\text{KV}} = T \cdot G_l \cdot \bigl(d_k^{(l)} + d_v^{(l)}\bigr),
\end{equation}
也是该层可使用的最大 rank。若全模型目标压缩率为 $\rho \in (0,1]$，则剩余的 rank 总量需满足 $\sum_l \bigl(r_{k,l}+r_{v,l}\bigr) \approx \rho \sum_l \bigl(d_k^{(l)}+d_v^{(l)}\bigr)$。

为了在这个约束下给每层的 Key/Value 分配初值，我们分别对 $M_{k,l}$ 与 $M_{v,l}$ 做 SVD。记其奇异值为 $\{\sigma_{k,l,i}\}_{i=1}^{d_k^{(l)}}$、$\{\sigma_{v,l,i}\}_{i=1}^{d_v^{(l)}}$。第 $i$ 个秩的相对能量占比分别为
\begin{equation}
    \label{eq:energy-weight-kv}
    w_{k,l,i} = \frac{\sigma_{k,l,i}^2}{\sum_{j=1}^{d_k^{(l)}} \sigma_{k,l,j}^2},\qquad
    w_{v,l,i} = \frac{\sigma_{v,l,i}^2}{\sum_{j=1}^{d_v^{(l)}} \sigma_{v,l,j}^2},
\end{equation}
并满足 $\sum_i w_{k,l,i} = \sum_i w_{v,l,i} = 1$。这些权重刻画了删除对应秩会损失的相对能量。

随后根据 Key 与 Value 在缓存中的占比分配各自的“保留能量份额”。定义
\begin{equation}
    C_{k,l}=T \cdot G_l \cdot d_k^{(l)},\qquad
    C_{v,l}=T \cdot G_l \cdot d_v^{(l)},\qquad
    C_{\text{tot}}=\sum_{j=1}^{L}(C_{k,j}+C_{v,j}),
\end{equation}
并令
\begin{equation}
    \label{eq:energy-budget-sep}
    \beta_{k,l}=\frac{C_{k,l}}{C_{\text{tot}}},\qquad
    \beta_{v,l}=\frac{C_{v,l}}{C_{\text{tot}}},\qquad
    \mathcal{E}_{k,l}^{\text{keep}}=\rho \cdot \beta_{k,l},\qquad
    \mathcal{E}_{v,l}^{\text{keep}}=\rho \cdot \beta_{v,l}.
\end{equation}
也就是说，Key/Value 规模越大的层，会被分配更高的保留能量阈值。把 $w_{k,l,i}$、$w_{v,l,i}$ 按降序排列之后，分别找到最小的 $r_{k,l}^{(0)}$、$r_{v,l}^{(0)}$ 使得
\begin{equation}
    \label{eq:init-rank-kv}
    \sum_{i=1}^{r_{k,l}^{(0)}} w_{k,l,i} \ge \mathcal{E}_{k,l}^{\text{keep}},\qquad
    \sum_{i=1}^{r_{v,l}^{(0)}} w_{v,l,i} \ge \mathcal{E}_{v,l}^{\text{keep}}.
\end{equation}
这样一来，尾部能量占比最小的奇异值自然被列为待裁剪的对象，直到累计丢弃的相对能量达到 $(1-\rho)$。合并所有层后，$\{r_{k,l}^{(0)},r_{v,l}^{(0)}\}$ 既满足全局压缩率要求，又忠实反映了 Key 与 Value 在不同层的低秩差异，为后续带有误差传递与敏感性调控的贪心迭代提供了扎实的起点。下一节将在此基础上进一步考虑层间误差累积对秩分配的约束。