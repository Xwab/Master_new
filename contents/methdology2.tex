\chapter{层间动态Rank分配与误差调控}
\label{chap:dynamic-rank}

上一章我们已经给出了基于激活/注意力重加权的低秩分解算法，能够在每一层内部找到与真实推理目标 $XW_k$、$A X W_v$ 最匹配的主奇异子空间。本章的方法第二部分在此基础上进一步回答“每一层应该保留多少秩”这一跨层问题：我们不再为所有层设定统一的压缩率，而是同时建模（1）分解目标自身的低秩性，刻画该层在联合 SVD 下主能量的集中程度，并用作 rank 初始化的先验；（2）对丢弃秩的敏感性，评估 Key/Value 在注意力反馈中的梯度放大与残差重建难度，用于指导跨层 rank 传递时的步长；（3）多层串联时低秩近似误差的累积风险，确保深层段的近似误差不会在长链路 KV 缓存里失控，并在不同层之间传递 rank 配额时提供约束信号。通过将这三类指标归一化为层级打分，并在全模型 rank 预算约束下运行一个逐步迭代的贪心分配过程，我们可以为每一层分配自适应的秩上限，保证激活/注意力感知的分解结果有足够的表达度，同时把全局误差保持在推理精度可接受的范围。接下来将从指标构建、分配算法与训练/推理期间的更新策略三个角度展开层间 rank 分配的具体流程。

\section{基于参数低秩性的层间秩初始化}
跨层 rank 调度的第一步是弄清楚“各层原始需要的秩”。设模型共有 $L$ 个注意力层，第 $l$ 层在 GQA 架构下含有 $G_l$ 个 KV 组，每组的 Key/Value 维度分别为 $d_k^{(l)}$ 和 $d_v^{(l)}$，推理缓存长度为 $T$。在不压缩的情况下，该层一次写入 KV 缓存的体积为
\begin{equation}
    \label{eq:kv-cost}
    \mathcal{C}_l^{\text{KV}} = T \cdot G_l \cdot \bigl(d_k^{(l)} + d_v^{(l)}\bigr),
\end{equation}
也是它最多可使用的 rank 数。若全模型目标压缩率为 $\rho \in (0,1]$，所有层剩余的秩之和需控制在 $\rho \sum_{l} (d_k^{(l)} + d_v^{(l)})$ 以内。

为了在这一约束下给每层分配初值，我们先对上一章构造的“激活/注意力重加权”矩阵
\[
M_{k,l} = S_{k,l} W_{k,l}, \qquad
M_{v,l} = S_{v,l} W_{v,l}
\]
做联合 SVD。记其奇异值为 $\{\sigma_{l,i}\}_{i=1}^{d_l}$，$d_l = d_k^{(l)} + d_v^{(l)}$。第 $i$ 个秩的相对能量写成
\begin{equation}
    \label{eq:energy-weight}
    w_{l,i} = \frac{\sigma_{l,i}^2}{\sum_{j=1}^{d_l} \sigma_{l,j}^2}, \qquad
    \sum_{i=1}^{d_l} w_{l,i} = 1,
\end{equation}
它直接衡量删除该奇异向量的代价。能量越高，说明该秩对重构误差的影响越大。

接下来根据每层原始 KV 体积来分配需要保留的能量份额。具体做法是计算
\begin{equation}
    \label{eq:energy-budget}
    \beta_l = \frac{\mathcal{C}_l^{\text{KV}}}{\sum_{j=1}^{L} \mathcal{C}_j^{\text{KV}}}, \qquad
    \mathcal{E}_l^{\text{keep}} = \rho \cdot \beta_l,
\end{equation}
其中 $\beta_l$ 表示层 $l$ 在总 KV 成本中的占比，$\mathcal{E}_l^{\text{keep}}$ 就是该层应当保留下来的能量份额。然后把 $\{w_{l,i}\}$ 从大到小排列，寻找使得
\begin{equation}
    \label{eq:init-rank}
    \sum_{i=1}^{r_l^{(0)}} w_{l,i} \ge \mathcal{E}_l^{\text{keep}}
\end{equation}
的最小 $r_l^{(0)}$，并把它作为层 $l$ 的初始秩上限。剩余能量 $\sum_{i=r_l^{(0)}+1}^{d_l} w_{l,i}$ 代表相对“不重要”的秩，被视为可以在初始化阶段直接裁剪的部分。

这一初始化策略等价于对所有层做一次统一阈值的能量截断：全局压缩率 $\rho$ 给出了应当至少保留多少能量，$\beta_l$ 则看齐了层之间的 KV 成本差异，使得 KV 代价高的层先保留更多主能量。最后得到的 $\{r_l^{(0)}\}$ 满足 $\sum_l r_l^{(0)} \approx \rho \sum_l d_l$，并且为后续基于误差累积和敏感性调整的贪心迭代提供了一个贴合统计特性的起点。下一节会在此基础上引入层间误差传递约束，对这些初值进行细化。