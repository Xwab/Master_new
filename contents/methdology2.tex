\chapter{层间动态Rank分配与误差调控}
\label{chap:dynamic-rank}

上一章我们已经给出了基于激活/注意力重加权的低秩分解算法，能够在每一层内部找到与真实推理目标 $XW_k$、$A X W_v$ 最匹配的主奇异子空间。本章的方法第二部分在此基础上进一步回答“每一层应该保留多少秩”这一跨层问题：我们不再为所有层设定统一的压缩率，而是同时建模（1）分解目标自身的低秩性，刻画该层在联合 SVD 下主能量的集中程度，并用作 rank 初始化的先验；（2）对丢弃秩的敏感性，评估 Key/Value 在注意力反馈中的梯度放大与残差重建难度，用于指导跨层 rank 传递时的步长；（3）多层串联时低秩近似误差的累积风险，确保深层段的近似误差不会在长链路 KV 缓存里失控，并在不同层之间传递 rank 配额时提供约束信号。通过将这三类指标归一化为层级打分，并在全模型 rank 预算约束下运行一个逐步迭代的贪心分配过程，我们可以为每一层分配自适应的秩上限，保证激活/注意力感知的分解结果有足够的表达度，同时把全局误差保持在推理精度可接受的范围。接下来将从指标构建、分配算法与训练/推理期间的更新策略三个角度展开层间 rank 分配的具体流程。